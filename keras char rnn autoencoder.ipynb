{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, RepeatVector, Embedding, Bidirectional, Activation, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import csv\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verbatim_to_char_seq(name, char_indices, maxLen):\n",
    "    name_chars = list(name)\n",
    "    name_chars_indices = list(map(lambda char: char_indices[char], name_chars))\n",
    "    return sequence.pad_sequences([name_chars_indices], maxLen, padding=\"post\", truncating=\"post\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import verbatims, put into two lists\n",
    "verbatims = []\n",
    "allowedChars = []\n",
    "with codecs.open('Data/Catagorization training data.csv', 'r', encoding='ascii', errors='ignore') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        temp = line[1].replace('\\n', ' ').replace('\\r', ' ').replace('<', ' ').replace('>', ' ').replace('*', ' ')\n",
    "        temp = temp.replace('%', ' ').replace('&', ' ').replace('#', ' ').replace('~', ' ').replace('@', ' ')\n",
    "        temp = temp.replace('=', ' ').replace('`', ' ').replace(';', ' ').replace('_', ' ').replace('+', ' ')\n",
    "        temp = (temp[:198] + '..') if len(temp) > 200 else temp\n",
    "        verbatims.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_list = list(set(''.join(verbatims)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(char_list))\n",
    "indices_char = dict((i, c) for i, c in enumerate(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of duplicates\n",
    "verbatims = list(set(verbatims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#determine the maximum length of the verbatims\n",
    "maxLen = 0\n",
    "for v in verbatims:\n",
    "    if len(v) > maxLen:\n",
    "        maxLen = len(v)\n",
    "print(maxLen)\n",
    "\n",
    "#if the max length is < 50, pad verbatim\n",
    "#if maxLen < 50:\n",
    "#    maxLen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118479, 200)\n"
     ]
    }
   ],
   "source": [
    "#create actual dataset to be fed into keras model\n",
    "X = []\n",
    "\n",
    "for n in verbatims:\n",
    "    X.append(verbatim_to_char_seq(n, char_indices, maxLen))\n",
    "    \n",
    "X = np.array(X).astype(np.uint8)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump((char_list, char_indices, indices_char, X), open('char_autoenc.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "char_list, char_indices, indices_char, X = pickle.load(open( 'char_autoenc.pkl', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "[[False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [ True False False ..., False False False]\n",
      " [ True False False ..., False False False]\n",
      " [False False False ..., False False False]]\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "maxLen = X.shape[1]\n",
    "import numpy as np\n",
    "\n",
    "Z = np.zeros((len(X), maxLen, len(char_list)), dtype=np.bool)\n",
    "for i, seq in enumerate(X):\n",
    "    for t, word in enumerate(seq[:-1]):\n",
    "        Z[i, t, word] = 1\n",
    "        \n",
    "print(Z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs (None, 200, 48)\n",
      "encoded (None, 64)\n",
      "decoded (None, 200, 64)\n",
      "output (None, 200, 48)\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "#create keras model\n",
    "batch_size = 512\n",
    "nb_epoch = 200\n",
    "\n",
    "latent_dim = 64\n",
    "\n",
    "timesteps = maxLen\n",
    "inputs = Input(shape=(maxLen, len(char_list)))\n",
    "print(\"inputs\", K.int_shape(inputs) )\n",
    "\n",
    "#(number of samples, number of timesteps, number of features)\n",
    "encoded = LSTM(latent_dim)(inputs)\n",
    "print(\"encoded\", K.int_shape(encoded))\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "print(\"decoded\", K.int_shape(decoded))\n",
    "\n",
    "decoded = LSTM(len(char_list), return_sequences=True)(decoded)\n",
    "print(\"output\", K.int_shape(decoded))\n",
    "\n",
    "# is sigmoid the best choice?\n",
    "activation = Activation('sigmoid')(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, activation)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "#optimizer = Adam(lr = 0.005)\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "sequence_autoencoder.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "#checkpointer = ModelCheckpoint(filepath='char_lstm2_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model2.load_weights('char_lstm2_keras_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106631 samples, validate on 11848 samples\n",
      "Epoch 1/200\n",
      "106631/106631 [==============================] - 159s - loss: 3.2301 - val_loss: 3.1295\n",
      "Epoch 2/200\n",
      "106631/106631 [==============================] - 157s - loss: 3.1282 - val_loss: 3.1245\n",
      "Epoch 3/200\n",
      "106631/106631 [==============================] - 157s - loss: 3.1241 - val_loss: 3.1206\n",
      "Epoch 4/200\n",
      "106631/106631 [==============================] - 157s - loss: 3.1128 - val_loss: 3.1057\n",
      "Epoch 5/200\n",
      "106631/106631 [==============================] - 157s - loss: 3.1052 - val_loss: 3.1017\n",
      "Epoch 6/200\n",
      " 36864/106631 [=========>....................] - ETA: 99s - loss: 3.1027 "
     ]
    }
   ],
   "source": [
    "#train model on test data\n",
    "sequence_autoencoder.fit(Z, Z, \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_autoencoder.save(\"seq2seq_model.h5\")\n",
    "#from keras.models import load_model\n",
    "#sequence_autoencoder = load_model(\"seq2seq_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
