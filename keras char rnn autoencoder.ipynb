{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, RepeatVector, Embedding, Bidirectional, Activation, Dense, Dropout, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import csv\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verbatim_to_char_seq(name, char_indices, maxLen):\n",
    "    name_chars = list(name)\n",
    "    name_chars_indices = list(map(lambda char: char_indices[char], name_chars))\n",
    "    return sequence.pad_sequences([name_chars_indices], maxLen, padding=\"post\", truncating=\"post\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import verbatims, put into two lists\n",
    "verbatims = []\n",
    "allowedChars = []\n",
    "with codecs.open('Data/Catagorization training data.csv', 'r', encoding='ascii', errors='ignore') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        temp = line[1].replace('\\n', ' ').replace('\\r', ' ').replace('<', ' ').replace('>', ' ').replace('*', ' ')\n",
    "        temp = temp.replace('%', ' ').replace('&', ' ').replace('#', ' ').replace('~', ' ').replace('@', ' ')\n",
    "        temp = temp.replace('=', ' ').replace('`', ' ').replace(';', ' ').replace('_', ' ').replace('+', ' ')\n",
    "        temp = (temp[:198] + '..') if len(temp) > 200 else temp\n",
    "        verbatims.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_list = list(set(''.join(verbatims)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(char_list))\n",
    "indices_char = dict((i, c) for i, c in enumerate(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of duplicates\n",
    "verbatims = list(set(verbatims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#determine the maximum length of the verbatims\n",
    "maxLen = 0\n",
    "for v in verbatims:\n",
    "    if len(v) > maxLen:\n",
    "        maxLen = len(v)\n",
    "print(maxLen)\n",
    "\n",
    "#if the max length is < 50, pad verbatim\n",
    "#if maxLen < 50:\n",
    "#    maxLen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118479, 200)\n"
     ]
    }
   ],
   "source": [
    "#create actual dataset to be fed into keras model\n",
    "X = []\n",
    "\n",
    "for n in verbatims:\n",
    "    X.append(verbatim_to_char_seq(n, char_indices, maxLen))\n",
    "    \n",
    "X = np.array(X).astype(np.uint8)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump((char_list, char_indices, indices_char, X), open('char_autoenc.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "char_list, char_indices, indices_char, X = pickle.load(open( 'char_autoenc.pkl', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "[[False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [ True False False ..., False False False]\n",
      " [ True False False ..., False False False]\n",
      " [False False False ..., False False False]]\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "maxLen = X.shape[1]\n",
    "import numpy as np\n",
    "\n",
    "Z = np.zeros((len(X), maxLen, len(char_list)), dtype=np.bool)\n",
    "for i, seq in enumerate(X):\n",
    "    for t, word in enumerate(seq[:-1]):\n",
    "        Z[i, t, word] = 1\n",
    "        \n",
    "print(Z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs (None, 200, 48)\n",
      "encoded1 (None, 200, 128)\n",
      "encoded2 (None, 64)\n",
      "decoded1 (None, 200, 64)\n",
      "decoded2 (None, 200, 64)\n",
      "decoded3 (None, 200, 128)\n",
      "output (None, 200, 48)\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "#create keras model\n",
    "batch_size = 256\n",
    "nb_epoch = 200\n",
    "\n",
    "latent_dim_out = 128\n",
    "latent_dim_in = 64\n",
    "\n",
    "timesteps = maxLen\n",
    "inputs = Input(shape=(maxLen, len(char_list)))\n",
    "print(\"inputs\", K.int_shape(inputs) )\n",
    "\n",
    "#(number of samples, number of timesteps, number of features)\n",
    "encoded1 = LSTM(latent_dim_out, return_sequences=True)(inputs)\n",
    "print(\"encoded1\", K.int_shape(encoded1))\n",
    "\n",
    "encoded2 = LSTM(latent_dim_in)(encoded1)\n",
    "print(\"encoded2\", K.int_shape(encoded2))\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded2)\n",
    "print(\"decoded1\", K.int_shape(decoded))\n",
    "\n",
    "decoded = LSTM(latent_dim_in, return_sequences=True)(decoded)\n",
    "print(\"decoded2\", K.int_shape(decoded))\n",
    "\n",
    "decoded = LSTM(latent_dim_out, return_sequences=True)(decoded)\n",
    "print(\"decoded3\", K.int_shape(decoded))\n",
    "\n",
    "decoded = Dropout(0.5)(decoded)\n",
    "decoded = TimeDistributed(Dense(len(char_list)), input_shape=(200, 128))(decoded)\n",
    "print(\"output\", K.int_shape(decoded))\n",
    "# is sigmoid the best choice?\n",
    "activation = Activation('sigmoid')(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, activation)\n",
    "encoder = Model(inputs, encoded2)\n",
    "\n",
    "#optimizer = Adam(lr = 0.005)\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "sequence_autoencoder.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "#checkpointer = ModelCheckpoint(filepath='char_lstm2_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model2.load_weights('char_lstm2_keras_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106631 samples, validate on 11848 samples\n",
      "Epoch 1/200\n",
      " 17152/106631 [===>..........................] - ETA: 470s - loss: 2.0720"
     ]
    }
   ],
   "source": [
    "#train model on test data\n",
    "sequence_autoencoder.fit(Z, Z, \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_autoencoder.save(\"seq2seq_model.h5\")\n",
    "#from keras.models import load_model\n",
    "#sequence_autoencoder = load_model(\"seq2seq_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs (None, 200, 48)\n",
      "output (None, 200, 64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "nb_epoch = 2000\n",
    "\n",
    "latent_dim_out = 128\n",
    "latent_dim_in = 64\n",
    "\n",
    "timesteps = maxLen\n",
    "inputs = Input(shape=(maxLen, len(char_list)))\n",
    "print(\"inputs\", K.int_shape(inputs) )\n",
    "\n",
    "decoded = LSTM(latent_dim_in, return_sequences=True)(inputs)\n",
    "print(\"output\", K.int_shape(decoded))\n",
    "decoded = Dropout(0.5)(decoded)\n",
    "decoded = TimeDistributed(Dense(len(char_list)), input_shape=(200, 48))(decoded)\n",
    "\n",
    "# is sigmoid the best choice?\n",
    "activation = Activation('sigmoid')(decoded)\n",
    "\n",
    "sequence_test = Model(inputs, activation)\n",
    "#encoder = Model(inputs, encoded2)\n",
    "\n",
    "#optimizer = Adam(lr = 0.005)\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "sequence_test.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45 samples, validate on 5 samples\n",
      "Epoch 1/2000\n",
      "45/45 [==============================] - 1s - loss: 3.8663 - val_loss: 3.8571\n",
      "Epoch 2/2000\n",
      "45/45 [==============================] - 0s - loss: 3.8589 - val_loss: 3.8498\n",
      "Epoch 3/2000\n",
      "45/45 [==============================] - 0s - loss: 3.8505 - val_loss: 3.8424\n",
      "Epoch 4/2000\n",
      "45/45 [==============================] - 0s - loss: 3.8427 - val_loss: 3.8350\n",
      "Epoch 5/2000\n",
      "45/45 [==============================] - 0s - loss: 3.8338 - val_loss: 3.8275\n",
      "Epoch 6/2000\n",
      "45/45 [==============================] - 0s - loss: 3.8265 - val_loss: 3.8197\n",
      "Epoch 7/2000\n",
      "45/45 [==============================] - 0s - loss: 3.8181 - val_loss: 3.8116\n",
      "Epoch 8/2000\n",
      "45/45 [==============================] - 0s - loss: 3.8091 - val_loss: 3.8031\n",
      "Epoch 9/2000\n",
      "45/45 [==============================] - 0s - loss: 3.8002 - val_loss: 3.7940\n",
      "Epoch 10/2000\n",
      "45/45 [==============================] - 0s - loss: 3.7907 - val_loss: 3.7841\n",
      "Epoch 11/2000\n",
      "45/45 [==============================] - 0s - loss: 3.7799 - val_loss: 3.7732\n",
      "Epoch 12/2000\n",
      "45/45 [==============================] - 0s - loss: 3.7678 - val_loss: 3.7610\n",
      "Epoch 13/2000\n",
      "45/45 [==============================] - 0s - loss: 3.7544 - val_loss: 3.7470\n",
      "Epoch 14/2000\n",
      "45/45 [==============================] - 0s - loss: 3.7400 - val_loss: 3.7307\n",
      "Epoch 15/2000\n",
      "45/45 [==============================] - 0s - loss: 3.7218 - val_loss: 3.7111\n",
      "Epoch 16/2000\n",
      "45/45 [==============================] - 0s - loss: 3.7010 - val_loss: 3.6871\n",
      "Epoch 17/2000\n",
      "45/45 [==============================] - 0s - loss: 3.6752 - val_loss: 3.6570\n",
      "Epoch 18/2000\n",
      "45/45 [==============================] - 0s - loss: 3.6427 - val_loss: 3.6194\n",
      "Epoch 19/2000\n",
      "45/45 [==============================] - 0s - loss: 3.6030 - val_loss: 3.5745\n",
      "Epoch 20/2000\n",
      "45/45 [==============================] - 0s - loss: 3.5549 - val_loss: 3.5260\n",
      "Epoch 21/2000\n",
      "45/45 [==============================] - 0s - loss: 3.5037 - val_loss: 3.4798\n",
      "Epoch 22/2000\n",
      "45/45 [==============================] - 0s - loss: 3.4540 - val_loss: 3.4395\n",
      "Epoch 23/2000\n",
      "45/45 [==============================] - 0s - loss: 3.4134 - val_loss: 3.4057\n",
      "Epoch 24/2000\n",
      "45/45 [==============================] - 0s - loss: 3.3796 - val_loss: 3.3765\n",
      "Epoch 25/2000\n",
      "45/45 [==============================] - 0s - loss: 3.3478 - val_loss: 3.3494\n",
      "Epoch 26/2000\n",
      "45/45 [==============================] - 0s - loss: 3.3202 - val_loss: 3.3232\n",
      "Epoch 27/2000\n",
      "45/45 [==============================] - 0s - loss: 3.2928 - val_loss: 3.2971\n",
      "Epoch 28/2000\n",
      "45/45 [==============================] - 0s - loss: 3.2644 - val_loss: 3.2705\n",
      "Epoch 29/2000\n",
      "45/45 [==============================] - 0s - loss: 3.2384 - val_loss: 3.2430\n",
      "Epoch 30/2000\n",
      "45/45 [==============================] - 0s - loss: 3.2112 - val_loss: 3.2143\n",
      "Epoch 31/2000\n",
      "45/45 [==============================] - 0s - loss: 3.1815 - val_loss: 3.1839\n",
      "Epoch 32/2000\n",
      "45/45 [==============================] - 0s - loss: 3.1499 - val_loss: 3.1516\n",
      "Epoch 33/2000\n",
      "45/45 [==============================] - 0s - loss: 3.1189 - val_loss: 3.1170\n",
      "Epoch 34/2000\n",
      "45/45 [==============================] - 0s - loss: 3.0851 - val_loss: 3.0799\n",
      "Epoch 35/2000\n",
      "45/45 [==============================] - 0s - loss: 3.0460 - val_loss: 3.0395\n",
      "Epoch 36/2000\n",
      "45/45 [==============================] - 0s - loss: 3.0082 - val_loss: 2.9958\n",
      "Epoch 37/2000\n",
      "45/45 [==============================] - 0s - loss: 2.9629 - val_loss: 2.9478\n",
      "Epoch 38/2000\n",
      "45/45 [==============================] - 0s - loss: 2.9163 - val_loss: 2.8953\n",
      "Epoch 39/2000\n",
      "45/45 [==============================] - 0s - loss: 2.8663 - val_loss: 2.8372\n",
      "Epoch 40/2000\n",
      "45/45 [==============================] - 0s - loss: 2.8122 - val_loss: 2.7732\n",
      "Epoch 41/2000\n",
      "45/45 [==============================] - 0s - loss: 2.7510 - val_loss: 2.7021\n",
      "Epoch 42/2000\n",
      "45/45 [==============================] - 0s - loss: 2.6807 - val_loss: 2.6225\n",
      "Epoch 43/2000\n",
      "45/45 [==============================] - 0s - loss: 2.6094 - val_loss: 2.5337\n",
      "Epoch 44/2000\n",
      "45/45 [==============================] - 0s - loss: 2.5215 - val_loss: 2.4337\n",
      "Epoch 45/2000\n",
      "45/45 [==============================] - 0s - loss: 2.4326 - val_loss: 2.3219\n",
      "Epoch 46/2000\n",
      "45/45 [==============================] - 0s - loss: 2.3212 - val_loss: 2.1994\n",
      "Epoch 47/2000\n",
      "45/45 [==============================] - 0s - loss: 2.2024 - val_loss: 2.0723\n",
      "Epoch 48/2000\n",
      "45/45 [==============================] - 0s - loss: 2.0837 - val_loss: 1.9481\n",
      "Epoch 49/2000\n",
      "45/45 [==============================] - 0s - loss: 1.9682 - val_loss: 1.8375\n",
      "Epoch 50/2000\n",
      "45/45 [==============================] - 0s - loss: 1.8507 - val_loss: 1.7460\n",
      "Epoch 51/2000\n",
      "45/45 [==============================] - 0s - loss: 1.7560 - val_loss: 1.6655\n",
      "Epoch 52/2000\n",
      "45/45 [==============================] - 0s - loss: 1.6777 - val_loss: 1.5986\n",
      "Epoch 53/2000\n",
      "45/45 [==============================] - 0s - loss: 1.5901 - val_loss: 1.5428\n",
      "Epoch 54/2000\n",
      "45/45 [==============================] - 0s - loss: 1.5300 - val_loss: 1.4977\n",
      "Epoch 55/2000\n",
      "45/45 [==============================] - 0s - loss: 1.4666 - val_loss: 1.4608\n",
      "Epoch 56/2000\n",
      "45/45 [==============================] - 0s - loss: 1.4127 - val_loss: 1.4298\n",
      "Epoch 57/2000\n",
      "45/45 [==============================] - 0s - loss: 1.3793 - val_loss: 1.4035\n",
      "Epoch 58/2000\n",
      "45/45 [==============================] - 0s - loss: 1.3350 - val_loss: 1.3814\n",
      "Epoch 59/2000\n",
      "45/45 [==============================] - 0s - loss: 1.3043 - val_loss: 1.3616\n",
      "Epoch 60/2000\n",
      "45/45 [==============================] - 0s - loss: 1.2702 - val_loss: 1.3429\n",
      "Epoch 61/2000\n",
      "45/45 [==============================] - 0s - loss: 1.2418 - val_loss: 1.3252\n",
      "Epoch 62/2000\n",
      "45/45 [==============================] - 0s - loss: 1.2204 - val_loss: 1.3057\n",
      "Epoch 63/2000\n",
      "45/45 [==============================] - 0s - loss: 1.1929 - val_loss: 1.2870\n",
      "Epoch 64/2000\n",
      "45/45 [==============================] - 0s - loss: 1.1670 - val_loss: 1.2683\n",
      "Epoch 65/2000\n",
      "45/45 [==============================] - 0s - loss: 1.1528 - val_loss: 1.2517\n",
      "Epoch 66/2000\n",
      "45/45 [==============================] - 0s - loss: 1.1392 - val_loss: 1.2636\n",
      "Epoch 67/2000\n",
      "45/45 [==============================] - 0s - loss: 1.1234 - val_loss: 1.2207\n",
      "Epoch 68/2000\n",
      "45/45 [==============================] - 0s - loss: 1.1046 - val_loss: 1.2317\n",
      "Epoch 69/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0873 - val_loss: 1.2966\n",
      "Epoch 70/2000\n",
      "45/45 [==============================] - 0s - loss: 1.2161 - val_loss: 1.2789\n",
      "Epoch 71/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0936 - val_loss: 1.2174\n",
      "Epoch 72/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0629 - val_loss: 1.1801\n",
      "Epoch 73/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0551 - val_loss: 1.2534\n",
      "Epoch 74/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0660 - val_loss: 1.1782\n",
      "Epoch 75/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0397 - val_loss: 1.2179\n",
      "Epoch 76/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0305 - val_loss: 1.1520\n",
      "Epoch 77/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0447 - val_loss: 1.2655\n",
      "Epoch 78/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0490 - val_loss: 1.1562\n",
      "Epoch 79/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0033 - val_loss: 1.1295\n",
      "Epoch 80/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9890 - val_loss: 1.1421\n",
      "Epoch 81/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9820 - val_loss: 1.1362\n",
      "Epoch 82/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9968 - val_loss: 1.2600\n",
      "Epoch 83/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0247 - val_loss: 1.1082\n",
      "Epoch 84/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9656 - val_loss: 1.1246\n",
      "Epoch 85/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9574 - val_loss: 1.1390\n",
      "Epoch 86/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9920 - val_loss: 1.2762\n",
      "Epoch 87/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0271 - val_loss: 1.1359\n",
      "Epoch 88/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9654 - val_loss: 1.0956\n",
      "Epoch 89/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s - loss: 0.9594 - val_loss: 1.1581\n",
      "Epoch 90/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9560 - val_loss: 1.1152\n",
      "Epoch 91/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9723 - val_loss: 1.2118\n",
      "Epoch 92/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9784 - val_loss: 1.1075\n",
      "Epoch 93/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9633 - val_loss: 1.2094\n",
      "Epoch 94/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9673 - val_loss: 1.0841\n",
      "Epoch 95/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9403 - val_loss: 1.1094\n",
      "Epoch 96/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9256 - val_loss: 1.0825\n",
      "Epoch 97/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9331 - val_loss: 1.1313\n",
      "Epoch 98/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9274 - val_loss: 1.4347\n",
      "Epoch 99/2000\n",
      "45/45 [==============================] - 0s - loss: 1.3588 - val_loss: 1.3610\n",
      "Epoch 100/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0623 - val_loss: 1.2887\n",
      "Epoch 101/2000\n",
      "45/45 [==============================] - 0s - loss: 1.0130 - val_loss: 1.1856\n",
      "Epoch 102/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9545 - val_loss: 1.0847\n",
      "Epoch 103/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9468 - val_loss: 1.1858\n",
      "Epoch 104/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9502 - val_loss: 1.0760\n",
      "Epoch 105/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9340 - val_loss: 1.1633\n",
      "Epoch 106/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9355 - val_loss: 1.0658\n",
      "Epoch 107/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9229 - val_loss: 1.1140\n",
      "Epoch 108/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9158 - val_loss: 1.0552\n",
      "Epoch 109/2000\n",
      "45/45 [==============================] - 0s - loss: 0.9046 - val_loss: 1.0828\n",
      "Epoch 110/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8982 - val_loss: 1.0458\n",
      "Epoch 111/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8906 - val_loss: 1.0579\n",
      "Epoch 112/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8849 - val_loss: 1.0403\n",
      "Epoch 113/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8821 - val_loss: 1.0411\n",
      "Epoch 114/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8768 - val_loss: 1.0351\n",
      "Epoch 115/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8721 - val_loss: 1.0337\n",
      "Epoch 116/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8714 - val_loss: 1.0289\n",
      "Epoch 117/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8699 - val_loss: 1.0259\n",
      "Epoch 118/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8659 - val_loss: 1.0244\n",
      "Epoch 119/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8662 - val_loss: 1.0193\n",
      "Epoch 120/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8601 - val_loss: 1.0197\n",
      "Epoch 121/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8587 - val_loss: 1.0155\n",
      "Epoch 122/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8561 - val_loss: 1.0135\n",
      "Epoch 123/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8521 - val_loss: 1.0111\n",
      "Epoch 124/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8490 - val_loss: 1.0095\n",
      "Epoch 125/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8502 - val_loss: 1.0065\n",
      "Epoch 126/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8485 - val_loss: 1.0051\n",
      "Epoch 127/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8457 - val_loss: 1.0029\n",
      "Epoch 128/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8395 - val_loss: 1.0022\n",
      "Epoch 129/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8417 - val_loss: 0.9985\n",
      "Epoch 130/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8391 - val_loss: 0.9979\n",
      "Epoch 131/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8335 - val_loss: 0.9957\n",
      "Epoch 132/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8352 - val_loss: 0.9932\n",
      "Epoch 133/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8349 - val_loss: 0.9928\n",
      "Epoch 134/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8338 - val_loss: 0.9904\n",
      "Epoch 135/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8321 - val_loss: 0.9878\n",
      "Epoch 136/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8286 - val_loss: 0.9889\n",
      "Epoch 137/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8293 - val_loss: 0.9839\n",
      "Epoch 138/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8282 - val_loss: 0.9901\n",
      "Epoch 139/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8257 - val_loss: 0.9826\n",
      "Epoch 140/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8251 - val_loss: 0.9925\n",
      "Epoch 141/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8268 - val_loss: 0.9846\n",
      "Epoch 142/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8294 - val_loss: 0.9862\n",
      "Epoch 143/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8211 - val_loss: 0.9772\n",
      "Epoch 144/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8219 - val_loss: 0.9819\n",
      "Epoch 145/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8178 - val_loss: 0.9733\n",
      "Epoch 146/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8182 - val_loss: 0.9780\n",
      "Epoch 147/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8138 - val_loss: 0.9700\n",
      "Epoch 148/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8122 - val_loss: 0.9767\n",
      "Epoch 149/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8111 - val_loss: 0.9682\n",
      "Epoch 150/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8130 - val_loss: 0.9730\n",
      "Epoch 151/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8089 - val_loss: 0.9651\n",
      "Epoch 152/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8108 - val_loss: 0.9691\n",
      "Epoch 153/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8078 - val_loss: 0.9621\n",
      "Epoch 154/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8064 - val_loss: 0.9677\n",
      "Epoch 155/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8064 - val_loss: 0.9600\n",
      "Epoch 156/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8063 - val_loss: 0.9653\n",
      "Epoch 157/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8005 - val_loss: 0.9574\n",
      "Epoch 158/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8023 - val_loss: 0.9614\n",
      "Epoch 159/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7979 - val_loss: 0.9547\n",
      "Epoch 160/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8021 - val_loss: 0.9587\n",
      "Epoch 161/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7991 - val_loss: 0.9525\n",
      "Epoch 162/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7983 - val_loss: 0.9590\n",
      "Epoch 163/2000\n",
      "45/45 [==============================] - 0s - loss: 0.8005 - val_loss: 0.9523\n",
      "Epoch 164/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7955 - val_loss: 0.9581\n",
      "Epoch 165/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7933 - val_loss: 0.9509\n",
      "Epoch 166/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7983 - val_loss: 0.9529\n",
      "Epoch 167/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7935 - val_loss: 0.9460\n",
      "Epoch 168/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7922 - val_loss: 0.9504\n",
      "Epoch 169/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7908 - val_loss: 0.9441\n",
      "Epoch 170/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7913 - val_loss: 0.9479\n",
      "Epoch 171/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7913 - val_loss: 0.9419\n",
      "Epoch 172/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7923 - val_loss: 0.9471\n",
      "Epoch 173/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7855 - val_loss: 0.9407\n",
      "Epoch 174/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7871 - val_loss: 0.9472\n",
      "Epoch 175/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7828 - val_loss: 0.9390\n",
      "Epoch 176/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7865 - val_loss: 0.9447\n",
      "Epoch 177/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s - loss: 0.7777 - val_loss: 0.9374\n",
      "Epoch 178/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7847 - val_loss: 0.9439\n",
      "Epoch 179/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7829 - val_loss: 0.9370\n",
      "Epoch 180/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7843 - val_loss: 0.9389\n",
      "Epoch 181/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7767 - val_loss: 0.9320\n",
      "Epoch 182/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7790 - val_loss: 0.9372\n",
      "Epoch 183/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7744 - val_loss: 0.9303\n",
      "Epoch 184/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7755 - val_loss: 0.9369\n",
      "Epoch 185/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7763 - val_loss: 0.9296\n",
      "Epoch 186/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7766 - val_loss: 0.9373\n",
      "Epoch 187/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7773 - val_loss: 0.9306\n",
      "Epoch 188/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7773 - val_loss: 0.9307\n",
      "Epoch 189/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7684 - val_loss: 0.9247\n",
      "Epoch 190/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7694 - val_loss: 0.9328\n",
      "Epoch 191/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7698 - val_loss: 0.9251\n",
      "Epoch 192/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7709 - val_loss: 0.9326\n",
      "Epoch 193/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7691 - val_loss: 0.9249\n",
      "Epoch 194/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7730 - val_loss: 0.9260\n",
      "Epoch 195/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7676 - val_loss: 0.9201\n",
      "Epoch 196/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7689 - val_loss: 0.9251\n",
      "Epoch 197/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7648 - val_loss: 0.9188\n",
      "Epoch 198/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7689 - val_loss: 0.9245\n",
      "Epoch 199/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7605 - val_loss: 0.9169\n",
      "Epoch 200/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7627 - val_loss: 0.9249\n",
      "Epoch 201/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7649 - val_loss: 0.9178\n",
      "Epoch 202/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7663 - val_loss: 0.9205\n",
      "Epoch 203/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7626 - val_loss: 0.9147\n",
      "Epoch 204/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7628 - val_loss: 0.9219\n",
      "Epoch 205/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7577 - val_loss: 0.9145\n",
      "Epoch 206/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7639 - val_loss: 0.9198\n",
      "Epoch 207/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7595 - val_loss: 0.9121\n",
      "Epoch 208/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7593 - val_loss: 0.9172\n",
      "Epoch 209/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7581 - val_loss: 0.9106\n",
      "Epoch 210/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7571 - val_loss: 0.9169\n",
      "Epoch 211/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7570 - val_loss: 0.9089\n",
      "Epoch 212/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7576 - val_loss: 0.9152\n",
      "Epoch 213/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7544 - val_loss: 0.9074\n",
      "Epoch 214/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7569 - val_loss: 0.9142\n",
      "Epoch 215/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7515 - val_loss: 0.9064\n",
      "Epoch 216/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7515 - val_loss: 0.9133\n",
      "Epoch 217/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7537 - val_loss: 0.9064\n",
      "Epoch 218/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7562 - val_loss: 0.9098\n",
      "Epoch 219/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7464 - val_loss: 0.9023\n",
      "Epoch 220/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7496 - val_loss: 0.9112\n",
      "Epoch 221/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7484 - val_loss: 0.9021\n",
      "Epoch 222/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7486 - val_loss: 0.9105\n",
      "Epoch 223/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7464 - val_loss: 0.9009\n",
      "Epoch 224/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7493 - val_loss: 0.9081\n",
      "Epoch 225/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7464 - val_loss: 0.9008\n",
      "Epoch 226/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7505 - val_loss: 0.9026\n",
      "Epoch 227/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7445 - val_loss: 0.8969\n",
      "Epoch 228/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7453 - val_loss: 0.9021\n",
      "Epoch 229/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7405 - val_loss: 0.8956\n",
      "Epoch 230/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7451 - val_loss: 0.9066\n",
      "Epoch 231/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7411 - val_loss: 0.8986\n",
      "Epoch 232/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7473 - val_loss: 0.8994\n",
      "Epoch 233/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7386 - val_loss: 0.8932\n",
      "Epoch 234/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7406 - val_loss: 0.9059\n",
      "Epoch 235/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7419 - val_loss: 0.8963\n",
      "Epoch 236/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7449 - val_loss: 0.8971\n",
      "Epoch 237/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7396 - val_loss: 0.8908\n",
      "Epoch 238/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7385 - val_loss: 0.9059\n",
      "Epoch 239/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7367 - val_loss: 0.8963\n",
      "Epoch 240/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7465 - val_loss: 0.8918\n",
      "Epoch 241/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7334 - val_loss: 0.8912\n",
      "Epoch 242/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7340 - val_loss: 0.8899\n",
      "Epoch 243/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7355 - val_loss: 0.8899\n",
      "Epoch 244/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7357 - val_loss: 0.8878\n",
      "Epoch 245/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7312 - val_loss: 0.8980\n",
      "Epoch 246/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7343 - val_loss: 0.8906\n",
      "Epoch 247/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7397 - val_loss: 0.8980\n",
      "Epoch 248/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7319 - val_loss: 0.8903\n",
      "Epoch 249/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7372 - val_loss: 0.8878\n",
      "Epoch 250/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7321 - val_loss: 0.8850\n",
      "Epoch 251/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7290 - val_loss: 0.8868\n",
      "Epoch 252/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7291 - val_loss: 0.8854\n",
      "Epoch 253/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7279 - val_loss: 0.8853\n",
      "Epoch 254/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7297 - val_loss: 0.8832\n",
      "Epoch 255/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7273 - val_loss: 0.9450\n",
      "Epoch 256/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7278 - val_loss: 0.9031\n",
      "Epoch 257/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7522 - val_loss: 0.8821\n",
      "Epoch 258/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7301 - val_loss: 0.8834\n",
      "Epoch 259/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7256 - val_loss: 0.8807\n",
      "Epoch 260/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7223 - val_loss: 0.8849\n",
      "Epoch 261/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7238 - val_loss: 0.8784\n",
      "Epoch 262/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7265 - val_loss: 0.8885\n",
      "Epoch 263/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7226 - val_loss: 0.8784\n",
      "Epoch 264/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7281 - val_loss: 0.8848\n",
      "Epoch 265/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s - loss: 0.7246 - val_loss: 0.8760\n",
      "Epoch 266/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7256 - val_loss: 0.9151\n",
      "Epoch 267/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7256 - val_loss: 0.8893\n",
      "Epoch 268/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7390 - val_loss: 0.8756\n",
      "Epoch 269/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7217 - val_loss: 0.8785\n",
      "Epoch 270/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7207 - val_loss: 0.8763\n",
      "Epoch 271/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7201 - val_loss: 0.8766\n",
      "Epoch 272/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7190 - val_loss: 0.8744\n",
      "Epoch 273/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7193 - val_loss: 0.8804\n",
      "Epoch 274/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7195 - val_loss: 0.8720\n",
      "Epoch 275/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7197 - val_loss: 0.8887\n",
      "Epoch 276/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7196 - val_loss: 0.8722\n",
      "Epoch 277/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7206 - val_loss: 0.8805\n",
      "Epoch 278/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7177 - val_loss: 0.8703\n",
      "Epoch 279/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7207 - val_loss: 0.8898\n",
      "Epoch 280/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7157 - val_loss: 0.8710\n",
      "Epoch 281/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7204 - val_loss: 0.8748\n",
      "Epoch 282/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7185 - val_loss: 0.8695\n",
      "Epoch 283/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7138 - val_loss: 0.8770\n",
      "Epoch 284/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7148 - val_loss: 0.8676\n",
      "Epoch 285/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7164 - val_loss: 0.9012\n",
      "Epoch 286/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7138 - val_loss: 0.8749\n",
      "Epoch 287/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7260 - val_loss: 0.8665\n",
      "Epoch 288/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7139 - val_loss: 0.8739\n",
      "Epoch 289/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7140 - val_loss: 0.8657\n",
      "Epoch 290/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7131 - val_loss: 0.8826\n",
      "Epoch 291/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7117 - val_loss: 0.8656\n",
      "Epoch 292/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7138 - val_loss: 0.8730\n",
      "Epoch 293/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7129 - val_loss: 0.8639\n",
      "Epoch 294/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7111 - val_loss: 0.8812\n",
      "Epoch 295/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7115 - val_loss: 0.8639\n",
      "Epoch 296/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7108 - val_loss: 0.8723\n",
      "Epoch 297/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7089 - val_loss: 0.8623\n",
      "Epoch 298/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7111 - val_loss: 0.8767\n",
      "Epoch 299/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7089 - val_loss: 0.8625\n",
      "Epoch 300/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7123 - val_loss: 0.8699\n",
      "Epoch 301/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7082 - val_loss: 0.8617\n",
      "Epoch 302/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7079 - val_loss: 0.8734\n",
      "Epoch 303/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7088 - val_loss: 0.8599\n",
      "Epoch 304/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7068 - val_loss: 0.8886\n",
      "Epoch 305/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7107 - val_loss: 0.8636\n",
      "Epoch 306/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7127 - val_loss: 0.8596\n",
      "Epoch 307/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7065 - val_loss: 0.8669\n",
      "Epoch 308/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7040 - val_loss: 0.8587\n",
      "Epoch 309/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7079 - val_loss: 0.8688\n",
      "Epoch 310/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7030 - val_loss: 0.8572\n",
      "Epoch 311/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7047 - val_loss: 0.8759\n",
      "Epoch 312/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7015 - val_loss: 0.8563\n",
      "Epoch 313/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7069 - val_loss: 0.8741\n",
      "Epoch 314/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7006 - val_loss: 0.8556\n",
      "Epoch 315/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7014 - val_loss: 0.8764\n",
      "Epoch 316/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7035 - val_loss: 0.8551\n",
      "Epoch 317/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7010 - val_loss: 0.8676\n",
      "Epoch 318/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7009 - val_loss: 0.8541\n",
      "Epoch 319/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7040 - val_loss: 0.8767\n",
      "Epoch 320/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7033 - val_loss: 0.8547\n",
      "Epoch 321/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7058 - val_loss: 0.8569\n",
      "Epoch 322/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7002 - val_loss: 0.8599\n",
      "Epoch 323/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6991 - val_loss: 0.8548\n",
      "Epoch 324/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6992 - val_loss: 0.8657\n",
      "Epoch 325/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6952 - val_loss: 0.8525\n",
      "Epoch 326/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6930 - val_loss: 0.9896\n",
      "Epoch 327/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7009 - val_loss: 0.8629\n",
      "Epoch 328/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7143 - val_loss: 0.8520\n",
      "Epoch 329/2000\n",
      "45/45 [==============================] - 0s - loss: 0.7024 - val_loss: 0.8504\n",
      "Epoch 330/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6995 - val_loss: 0.8530\n",
      "Epoch 331/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6987 - val_loss: 0.8519\n",
      "Epoch 332/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6971 - val_loss: 0.8529\n",
      "Epoch 333/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6961 - val_loss: 0.8517\n",
      "Epoch 334/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6933 - val_loss: 0.8536\n",
      "Epoch 335/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6953 - val_loss: 0.8514\n",
      "Epoch 336/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6943 - val_loss: 0.8546\n",
      "Epoch 337/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6934 - val_loss: 0.8506\n",
      "Epoch 338/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6957 - val_loss: 0.8564\n",
      "Epoch 339/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6917 - val_loss: 0.8486\n",
      "Epoch 340/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6932 - val_loss: 0.8583\n",
      "Epoch 341/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6902 - val_loss: 0.8462\n",
      "Epoch 342/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6949 - val_loss: 0.8649\n",
      "Epoch 343/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6910 - val_loss: 0.8447\n",
      "Epoch 344/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6923 - val_loss: 0.8716\n",
      "Epoch 345/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6923 - val_loss: 0.8442\n",
      "Epoch 346/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6947 - val_loss: 0.8516\n",
      "Epoch 347/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6907 - val_loss: 0.8501\n",
      "Epoch 348/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6893 - val_loss: 0.8513\n",
      "Epoch 349/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6909 - val_loss: 0.8517\n",
      "Epoch 350/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6903 - val_loss: 0.8452\n",
      "Epoch 351/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6856 - val_loss: 0.8838\n",
      "Epoch 352/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6879 - val_loss: 0.8420\n",
      "Epoch 353/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s - loss: 0.6899 - val_loss: 0.8502\n",
      "Epoch 354/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6880 - val_loss: 0.8435\n",
      "Epoch 355/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6894 - val_loss: 0.8641\n",
      "Epoch 356/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6860 - val_loss: 0.8398\n",
      "Epoch 357/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6881 - val_loss: 0.8611\n",
      "Epoch 358/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6884 - val_loss: 0.8398\n",
      "Epoch 359/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6874 - val_loss: 0.8597\n",
      "Epoch 360/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6834 - val_loss: 0.8398\n",
      "Epoch 361/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6844 - val_loss: 0.8667\n",
      "Epoch 362/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6849 - val_loss: 0.8374\n",
      "Epoch 363/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6857 - val_loss: 0.8547\n",
      "Epoch 364/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6838 - val_loss: 0.8389\n",
      "Epoch 365/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6852 - val_loss: 0.8625\n",
      "Epoch 366/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6827 - val_loss: 0.8358\n",
      "Epoch 367/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6869 - val_loss: 0.8488\n",
      "Epoch 368/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6834 - val_loss: 0.8429\n",
      "Epoch 369/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6819 - val_loss: 0.8461\n",
      "Epoch 370/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6816 - val_loss: 0.8413\n",
      "Epoch 371/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6806 - val_loss: 0.8617\n",
      "Epoch 372/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6819 - val_loss: 0.8335\n",
      "Epoch 373/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6796 - val_loss: 0.8698\n",
      "Epoch 374/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6857 - val_loss: 0.8334\n",
      "Epoch 375/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6864 - val_loss: 0.8405\n",
      "Epoch 376/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6810 - val_loss: 0.8411\n",
      "Epoch 377/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6806 - val_loss: 0.8429\n",
      "Epoch 378/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6789 - val_loss: 0.8435\n",
      "Epoch 379/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6780 - val_loss: 0.8408\n",
      "Epoch 380/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6788 - val_loss: 0.8444\n",
      "Epoch 381/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6792 - val_loss: 0.8398\n",
      "Epoch 382/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6762 - val_loss: 0.8583\n",
      "Epoch 383/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6783 - val_loss: 0.8293\n",
      "Epoch 384/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6778 - val_loss: 0.8637\n",
      "Epoch 385/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6779 - val_loss: 0.8285\n",
      "Epoch 386/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6784 - val_loss: 0.8598\n",
      "Epoch 387/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6768 - val_loss: 0.8276\n",
      "Epoch 388/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6771 - val_loss: 0.8504\n",
      "Epoch 389/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6756 - val_loss: 0.8278\n",
      "Epoch 390/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6773 - val_loss: 0.8653\n",
      "Epoch 391/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6725 - val_loss: 0.8257\n",
      "Epoch 392/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6784 - val_loss: 0.8524\n",
      "Epoch 393/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6732 - val_loss: 0.8264\n",
      "Epoch 394/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6741 - val_loss: 0.8496\n",
      "Epoch 395/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6734 - val_loss: 0.8257\n",
      "Epoch 396/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6764 - val_loss: 0.8699\n",
      "Epoch 397/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6747 - val_loss: 0.8242\n",
      "Epoch 398/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6772 - val_loss: 0.8337\n",
      "Epoch 399/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6707 - val_loss: 0.8299\n",
      "Epoch 400/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6708 - val_loss: 0.8390\n",
      "Epoch 401/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6713 - val_loss: 0.8255\n",
      "Epoch 402/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6728 - val_loss: 0.8461\n",
      "Epoch 403/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6723 - val_loss: 0.8215\n",
      "Epoch 404/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6703 - val_loss: 0.8633\n",
      "Epoch 405/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6667 - val_loss: 0.8201\n",
      "Epoch 406/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6730 - val_loss: 0.8335\n",
      "Epoch 407/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6688 - val_loss: 0.8239\n",
      "Epoch 408/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6658 - val_loss: 0.8425\n",
      "Epoch 409/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6670 - val_loss: 0.8188\n",
      "Epoch 410/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6663 - val_loss: 0.8589\n",
      "Epoch 411/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6657 - val_loss: 0.8172\n",
      "Epoch 412/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6707 - val_loss: 0.8316\n",
      "Epoch 413/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6665 - val_loss: 0.8222\n",
      "Epoch 414/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6643 - val_loss: 0.8455\n",
      "Epoch 415/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6706 - val_loss: 0.8155\n",
      "Epoch 416/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6691 - val_loss: 0.8380\n",
      "Epoch 417/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6650 - val_loss: 0.8158\n",
      "Epoch 418/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6660 - val_loss: 0.8419\n",
      "Epoch 419/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6676 - val_loss: 0.8136\n",
      "Epoch 420/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6671 - val_loss: 0.8325\n",
      "Epoch 421/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6628 - val_loss: 0.8159\n",
      "Epoch 422/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6640 - val_loss: 0.8302\n",
      "Epoch 423/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6638 - val_loss: 0.8187\n",
      "Epoch 424/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6638 - val_loss: 0.8424\n",
      "Epoch 425/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6608 - val_loss: 0.8115\n",
      "Epoch 426/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6635 - val_loss: 0.8476\n",
      "Epoch 427/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6612 - val_loss: 0.8107\n",
      "Epoch 428/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6651 - val_loss: 0.8262\n",
      "Epoch 429/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6590 - val_loss: 0.8156\n",
      "Epoch 430/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6633 - val_loss: 0.8266\n",
      "Epoch 431/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6591 - val_loss: 0.8112\n",
      "Epoch 432/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6588 - val_loss: 0.8404\n",
      "Epoch 433/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6599 - val_loss: 0.8072\n",
      "Epoch 434/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6630 - val_loss: 0.8310\n",
      "Epoch 435/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6606 - val_loss: 0.8067\n",
      "Epoch 436/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6603 - val_loss: 0.8336\n",
      "Epoch 437/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6587 - val_loss: 0.8051\n",
      "Epoch 438/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6605 - val_loss: 0.8238\n",
      "Epoch 439/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6570 - val_loss: 0.8051\n",
      "Epoch 440/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6596 - val_loss: 0.8336\n",
      "Epoch 441/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s - loss: 0.6583 - val_loss: 0.8033\n",
      "Epoch 442/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6604 - val_loss: 0.8157\n",
      "Epoch 443/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6573 - val_loss: 0.8076\n",
      "Epoch 444/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6564 - val_loss: 0.8135\n",
      "Epoch 445/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6533 - val_loss: 0.8069\n",
      "Epoch 446/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6537 - val_loss: 0.8185\n",
      "Epoch 447/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6543 - val_loss: 0.8016\n",
      "Epoch 448/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6546 - val_loss: 0.8232\n",
      "Epoch 449/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6563 - val_loss: 0.7992\n",
      "Epoch 450/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6571 - val_loss: 0.8177\n",
      "Epoch 451/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6508 - val_loss: 0.7997\n",
      "Epoch 452/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6541 - val_loss: 0.8241\n",
      "Epoch 453/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6522 - val_loss: 0.7970\n",
      "Epoch 454/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6523 - val_loss: 0.8193\n",
      "Epoch 455/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6540 - val_loss: 0.7962\n",
      "Epoch 456/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6503 - val_loss: 0.8213\n",
      "Epoch 457/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6508 - val_loss: 0.7954\n",
      "Epoch 458/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6527 - val_loss: 0.8108\n",
      "Epoch 459/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6495 - val_loss: 0.7947\n",
      "Epoch 460/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6503 - val_loss: 0.8125\n",
      "Epoch 461/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6503 - val_loss: 0.7935\n",
      "Epoch 462/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6504 - val_loss: 0.8132\n",
      "Epoch 463/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6468 - val_loss: 0.7917\n",
      "Epoch 464/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6499 - val_loss: 0.8037\n",
      "Epoch 465/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6470 - val_loss: 0.7937\n",
      "Epoch 466/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6460 - val_loss: 0.8100\n",
      "Epoch 467/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6477 - val_loss: 0.7895\n",
      "Epoch 468/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6484 - val_loss: 0.8052\n",
      "Epoch 469/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6479 - val_loss: 0.7882\n",
      "Epoch 470/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6477 - val_loss: 0.8028\n",
      "Epoch 471/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6456 - val_loss: 0.7872\n",
      "Epoch 472/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6463 - val_loss: 0.8070\n",
      "Epoch 473/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6431 - val_loss: 0.7860\n",
      "Epoch 474/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6464 - val_loss: 0.7989\n",
      "Epoch 475/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6432 - val_loss: 0.7850\n",
      "Epoch 476/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6448 - val_loss: 0.7974\n",
      "Epoch 477/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6411 - val_loss: 0.7832\n",
      "Epoch 478/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6425 - val_loss: 0.7958\n",
      "Epoch 479/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6420 - val_loss: 0.7828\n",
      "Epoch 480/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6429 - val_loss: 0.7955\n",
      "Epoch 481/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6402 - val_loss: 0.7813\n",
      "Epoch 482/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6425 - val_loss: 0.7933\n",
      "Epoch 483/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6403 - val_loss: 0.7802\n",
      "Epoch 484/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6389 - val_loss: 0.7960\n",
      "Epoch 485/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6400 - val_loss: 0.7783\n",
      "Epoch 486/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6390 - val_loss: 0.7901\n",
      "Epoch 487/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6392 - val_loss: 0.7781\n",
      "Epoch 488/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6391 - val_loss: 0.7939\n",
      "Epoch 489/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6362 - val_loss: 0.7763\n",
      "Epoch 490/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6403 - val_loss: 0.7881\n",
      "Epoch 491/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6382 - val_loss: 0.7744\n",
      "Epoch 492/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6369 - val_loss: 0.7820\n",
      "Epoch 493/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6349 - val_loss: 0.7739\n",
      "Epoch 494/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6373 - val_loss: 0.7790\n",
      "Epoch 495/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6361 - val_loss: 0.7726\n",
      "Epoch 496/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6372 - val_loss: 0.7789\n",
      "Epoch 497/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6325 - val_loss: 0.7719\n",
      "Epoch 498/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6325 - val_loss: 0.7762\n",
      "Epoch 499/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6320 - val_loss: 0.7726\n",
      "Epoch 500/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6342 - val_loss: 0.7717\n",
      "Epoch 501/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6333 - val_loss: 0.7741\n",
      "Epoch 502/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6305 - val_loss: 0.7686\n",
      "Epoch 503/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6298 - val_loss: 0.7789\n",
      "Epoch 504/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6311 - val_loss: 0.7661\n",
      "Epoch 505/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6322 - val_loss: 0.7775\n",
      "Epoch 506/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6297 - val_loss: 0.7649\n",
      "Epoch 507/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6286 - val_loss: 0.7744\n",
      "Epoch 508/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6275 - val_loss: 0.7632\n",
      "Epoch 509/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6307 - val_loss: 0.7725\n",
      "Epoch 510/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6302 - val_loss: 0.7625\n",
      "Epoch 511/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6296 - val_loss: 0.7715\n",
      "Epoch 512/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6275 - val_loss: 0.7597\n",
      "Epoch 513/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6283 - val_loss: 0.7680\n",
      "Epoch 514/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6254 - val_loss: 0.7602\n",
      "Epoch 515/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6309 - val_loss: 0.7644\n",
      "Epoch 516/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6237 - val_loss: 0.7563\n",
      "Epoch 517/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6230 - val_loss: 0.7664\n",
      "Epoch 518/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6266 - val_loss: 0.7557\n",
      "Epoch 519/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6222 - val_loss: 0.7603\n",
      "Epoch 520/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6188 - val_loss: 0.7535\n",
      "Epoch 521/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6236 - val_loss: 0.7573\n",
      "Epoch 522/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6219 - val_loss: 0.7521\n",
      "Epoch 523/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6250 - val_loss: 0.7557\n",
      "Epoch 524/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6230 - val_loss: 0.7510\n",
      "Epoch 525/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6238 - val_loss: 0.7554\n",
      "Epoch 526/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6230 - val_loss: 0.7490\n",
      "Epoch 527/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6212 - val_loss: 0.7572\n",
      "Epoch 528/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6202 - val_loss: 0.7472\n",
      "Epoch 529/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s - loss: 0.6208 - val_loss: 0.7532\n",
      "Epoch 530/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6167 - val_loss: 0.7454\n",
      "Epoch 531/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6196 - val_loss: 0.7513\n",
      "Epoch 532/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6191 - val_loss: 0.7439\n",
      "Epoch 533/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6173 - val_loss: 0.7499\n",
      "Epoch 534/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6174 - val_loss: 0.7425\n",
      "Epoch 535/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6146 - val_loss: 0.7469\n",
      "Epoch 536/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6161 - val_loss: 0.7404\n",
      "Epoch 537/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6161 - val_loss: 0.7422\n",
      "Epoch 538/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6137 - val_loss: 0.7389\n",
      "Epoch 539/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6116 - val_loss: 0.7412\n",
      "Epoch 540/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6117 - val_loss: 0.7366\n",
      "Epoch 541/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6134 - val_loss: 0.7401\n",
      "Epoch 542/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6151 - val_loss: 0.7348\n",
      "Epoch 543/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6122 - val_loss: 0.7387\n",
      "Epoch 544/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6094 - val_loss: 0.7335\n",
      "Epoch 545/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6091 - val_loss: 0.7361\n",
      "Epoch 546/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6068 - val_loss: 0.7340\n",
      "Epoch 547/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6093 - val_loss: 0.7358\n",
      "Epoch 548/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6072 - val_loss: 0.7291\n",
      "Epoch 549/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6052 - val_loss: 0.7328\n",
      "Epoch 550/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6027 - val_loss: 0.7266\n",
      "Epoch 551/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6100 - val_loss: 0.7296\n",
      "Epoch 552/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6051 - val_loss: 0.7259\n",
      "Epoch 553/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6021 - val_loss: 0.7276\n",
      "Epoch 554/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6019 - val_loss: 0.7238\n",
      "Epoch 555/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6011 - val_loss: 0.7259\n",
      "Epoch 556/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6020 - val_loss: 0.7224\n",
      "Epoch 557/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6015 - val_loss: 0.7220\n",
      "Epoch 558/2000\n",
      "45/45 [==============================] - 0s - loss: 0.6000 - val_loss: 0.7188\n",
      "Epoch 559/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5976 - val_loss: 0.7233\n",
      "Epoch 560/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5983 - val_loss: 0.7172\n",
      "Epoch 561/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5978 - val_loss: 0.7200\n",
      "Epoch 562/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5997 - val_loss: 0.7149\n",
      "Epoch 563/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5982 - val_loss: 0.7155\n",
      "Epoch 564/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5936 - val_loss: 0.7109\n",
      "Epoch 565/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5956 - val_loss: 0.7124\n",
      "Epoch 566/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5944 - val_loss: 0.7096\n",
      "Epoch 567/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5937 - val_loss: 0.7100\n",
      "Epoch 568/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5911 - val_loss: 0.7083\n",
      "Epoch 569/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5931 - val_loss: 0.7100\n",
      "Epoch 570/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5898 - val_loss: 0.7059\n",
      "Epoch 571/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5929 - val_loss: 0.7081\n",
      "Epoch 572/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5881 - val_loss: 0.7037\n",
      "Epoch 573/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5916 - val_loss: 0.7043\n",
      "Epoch 574/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5833 - val_loss: 0.6995\n",
      "Epoch 575/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5858 - val_loss: 0.7007\n",
      "Epoch 576/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5863 - val_loss: 0.6984\n",
      "Epoch 577/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5854 - val_loss: 0.6990\n",
      "Epoch 578/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5831 - val_loss: 0.6953\n",
      "Epoch 579/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5846 - val_loss: 0.6980\n",
      "Epoch 580/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5821 - val_loss: 0.6935\n",
      "Epoch 581/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5840 - val_loss: 0.6942\n",
      "Epoch 582/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5777 - val_loss: 0.6901\n",
      "Epoch 583/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5814 - val_loss: 0.6903\n",
      "Epoch 584/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5803 - val_loss: 0.6888\n",
      "Epoch 585/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5764 - val_loss: 0.6887\n",
      "Epoch 586/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5775 - val_loss: 0.6870\n",
      "Epoch 587/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5780 - val_loss: 0.6857\n",
      "Epoch 588/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5734 - val_loss: 0.6842\n",
      "Epoch 589/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5755 - val_loss: 0.6834\n",
      "Epoch 590/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5772 - val_loss: 0.6812\n",
      "Epoch 591/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5714 - val_loss: 0.6803\n",
      "Epoch 592/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5725 - val_loss: 0.6781\n",
      "Epoch 593/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5701 - val_loss: 0.6779\n",
      "Epoch 594/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5708 - val_loss: 0.6757\n",
      "Epoch 595/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5723 - val_loss: 0.6769\n",
      "Epoch 596/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5681 - val_loss: 0.6734\n",
      "Epoch 597/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5696 - val_loss: 0.6736\n",
      "Epoch 598/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5687 - val_loss: 0.6707\n",
      "Epoch 599/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5675 - val_loss: 0.6700\n",
      "Epoch 600/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5664 - val_loss: 0.6675\n",
      "Epoch 601/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5620 - val_loss: 0.6686\n",
      "Epoch 602/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5645 - val_loss: 0.6660\n",
      "Epoch 603/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5622 - val_loss: 0.6651\n",
      "Epoch 604/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5642 - val_loss: 0.6630\n",
      "Epoch 605/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5623 - val_loss: 0.6629\n",
      "Epoch 606/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5581 - val_loss: 0.6593\n",
      "Epoch 607/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5595 - val_loss: 0.6590\n",
      "Epoch 608/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5567 - val_loss: 0.6559\n",
      "Epoch 609/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5591 - val_loss: 0.6560\n",
      "Epoch 610/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5558 - val_loss: 0.6531\n",
      "Epoch 611/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5528 - val_loss: 0.6535\n",
      "Epoch 612/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5540 - val_loss: 0.6517\n",
      "Epoch 613/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5531 - val_loss: 0.6522\n",
      "Epoch 614/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5522 - val_loss: 0.6507\n",
      "Epoch 615/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5536 - val_loss: 0.6485\n",
      "Epoch 616/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5488 - val_loss: 0.6457\n",
      "Epoch 617/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s - loss: 0.5501 - val_loss: 0.6461\n",
      "Epoch 618/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5511 - val_loss: 0.6438\n",
      "Epoch 619/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5453 - val_loss: 0.6434\n",
      "Epoch 620/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5456 - val_loss: 0.6408\n",
      "Epoch 621/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5442 - val_loss: 0.6392\n",
      "Epoch 622/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5412 - val_loss: 0.6372\n",
      "Epoch 623/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5457 - val_loss: 0.6369\n",
      "Epoch 624/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5425 - val_loss: 0.6350\n",
      "Epoch 625/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5386 - val_loss: 0.6351\n",
      "Epoch 626/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5432 - val_loss: 0.6329\n",
      "Epoch 627/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5433 - val_loss: 0.6322\n",
      "Epoch 628/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5412 - val_loss: 0.6304\n",
      "Epoch 629/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5346 - val_loss: 0.6293\n",
      "Epoch 630/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5345 - val_loss: 0.6262\n",
      "Epoch 631/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5351 - val_loss: 0.6250\n",
      "Epoch 632/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5317 - val_loss: 0.6229\n",
      "Epoch 633/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5331 - val_loss: 0.6223\n",
      "Epoch 634/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5305 - val_loss: 0.6192\n",
      "Epoch 635/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5295 - val_loss: 0.6194\n",
      "Epoch 636/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5328 - val_loss: 0.6177\n",
      "Epoch 637/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5280 - val_loss: 0.6172\n",
      "Epoch 638/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5288 - val_loss: 0.6144\n",
      "Epoch 639/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5265 - val_loss: 0.6145\n",
      "Epoch 640/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5255 - val_loss: 0.6116\n",
      "Epoch 641/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5268 - val_loss: 0.6118\n",
      "Epoch 642/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5241 - val_loss: 0.6090\n",
      "Epoch 643/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5234 - val_loss: 0.6075\n",
      "Epoch 644/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5190 - val_loss: 0.6056\n",
      "Epoch 645/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5175 - val_loss: 0.6043\n",
      "Epoch 646/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5207 - val_loss: 0.6021\n",
      "Epoch 647/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5168 - val_loss: 0.6007\n",
      "Epoch 648/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5212 - val_loss: 0.5993\n",
      "Epoch 649/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5144 - val_loss: 0.5987\n",
      "Epoch 650/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5137 - val_loss: 0.5960\n",
      "Epoch 651/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5109 - val_loss: 0.5963\n",
      "Epoch 652/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5110 - val_loss: 0.5944\n",
      "Epoch 653/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5114 - val_loss: 0.5915\n",
      "Epoch 654/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5106 - val_loss: 0.5888\n",
      "Epoch 655/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5062 - val_loss: 0.5892\n",
      "Epoch 656/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5095 - val_loss: 0.5868\n",
      "Epoch 657/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5066 - val_loss: 0.5859\n",
      "Epoch 658/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5065 - val_loss: 0.5828\n",
      "Epoch 659/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5059 - val_loss: 0.5827\n",
      "Epoch 660/2000\n",
      "45/45 [==============================] - 0s - loss: 0.5026 - val_loss: 0.5800\n",
      "Epoch 661/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-f2758653a86d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           validation_split=0.1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1153\u001b[0m                         val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1154\u001b[0m                                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m                                                    verbose=0)\n\u001b[0m\u001b[1;32m   1156\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                             \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train model on test data\n",
    "sequence_test.fit(Z[:50,:,:], Z[:50,:,:], \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 200, 48)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[:50,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sequence_test.predict(Z[:5,:,:], batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.98337483e-01,   4.90137100e-01,   3.45884711e-01, ...,\n",
       "          4.08656895e-01,   3.80940765e-01,   3.29707295e-01],\n",
       "       [  3.45881760e-01,   3.83731902e-01,   1.94581002e-01, ...,\n",
       "          2.80710995e-01,   2.56413698e-01,   1.79367974e-01],\n",
       "       [  2.25228444e-01,   2.98587948e-01,   8.03020746e-02, ...,\n",
       "          1.83218688e-01,   1.38501197e-01,   6.63820133e-02],\n",
       "       ..., \n",
       "       [  9.15932775e-01,   3.00267748e-05,   5.22308846e-06, ...,\n",
       "          1.65851761e-05,   1.80058287e-05,   3.69853092e-06],\n",
       "       [  9.15932894e-01,   3.00267438e-05,   5.22307846e-06, ...,\n",
       "          1.65851761e-05,   1.80058105e-05,   3.69853092e-06],\n",
       "       [  9.10682678e-01,   3.26108966e-05,   5.41521194e-06, ...,\n",
       "          1.72657037e-05,   1.88223567e-05,   3.84939995e-06]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ..., \n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.64494971827662"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=(preds[0]*Z[0])\n",
    "s=0\n",
    "for i in a:\n",
    "    s += i.max()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55322474859138315"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f085995c780>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAEICAYAAACdyboFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERERFJEhZWIiIhIiqiwEhEREUmR/w98gWZkrelHhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f085a4c0a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Training history\")\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(sequence_test.history.history['loss'])\n",
    "ax1.set_title('loss')\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "plt.plot(sequence_test.history.history['val_loss'])\n",
    "ax2.set_title('validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
