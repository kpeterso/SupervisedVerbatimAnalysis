{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, LSTM, GRU, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import csv\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verbatim_to_char_seq(name, char_indices, maxLen):\n",
    "    name_chars = list(name)\n",
    "    name_chars_indices = list(map(lambda char: char_indices[char], name_chars))\n",
    "    return sequence.pad_sequences([name_chars_indices], maxLen)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import verbatims, put into two lists\n",
    "verbatims = []\n",
    "labels = []\n",
    "allowedChars = []\n",
    "with codecs.open('Data/Catagorization training data.csv', 'r', encoding='ascii', errors='ignore') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        temp = line[1].replace('\\n', ' ').replace('\\r', ' ').replace('<', ' ').replace('>', ' ').replace('*', ' ')\n",
    "        temp = temp.replace('%', ' ').replace('&', ' ').replace('#', ' ').replace('~', ' ').replace('@', ' ')\n",
    "        temp = temp.replace('=', ' ').replace('`', ' ').replace(';', ' ').replace('_', ' ').replace('+', ' ')\n",
    "        temp = (temp[:198] + '..') if len(temp) > 200 else temp\n",
    "        verbatims.append(temp)\n",
    "        labels.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_list = list(set(''.join(verbatims)))\n",
    "labels_list = list(set(labels))\n",
    "char_indices = dict((c, i) for i, c in enumerate(char_list))\n",
    "indices_char = dict((i, c) for i, c in enumerate(char_list))\n",
    "label_indices = dict((l, i) for i, l in enumerate(labels_list))\n",
    "indices_label = dict((i, l) for i, l in enumerate(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of duplicates and blanks\n",
    "\n",
    "objs = []\n",
    "for obj in list(zip(verbatims, labels)):\n",
    "    if len(obj[0].strip()) != 0:\n",
    "        objs.append(obj)\n",
    "\n",
    "objs = list(set(objs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118641\n"
     ]
    }
   ],
   "source": [
    "#separate out verbatims and labels again\n",
    "verbatims = []\n",
    "labels = []\n",
    "\n",
    "for n, l in objs:\n",
    "    verbatims.append(n)\n",
    "    labels.append(l)\n",
    "    \n",
    "print(len(verbatims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#determine the maximum length of the verbatims\n",
    "maxLen = 0\n",
    "for v in verbatims:\n",
    "    if len(v) > maxLen:\n",
    "        maxLen = len(v)\n",
    "print(maxLen)\n",
    "\n",
    "#if the max length is < 50, pad verbatim\n",
    "#if maxLen < 50:\n",
    "#    maxLen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118641, 200) (118641, 119)\n"
     ]
    }
   ],
   "source": [
    "#create actual dataset to be fed into keras model\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for n, l in zip(verbatims, labels):\n",
    "    X.append(verbatim_to_char_seq(n, char_indices, maxLen))\n",
    "    y.append(label_indices[l])\n",
    "    \n",
    "X = np.array(X).astype(np.uint8)\n",
    "y=np.array(y)\n",
    "y = np_utils.to_categorical(np.array(y)).astype(np.bool)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "#y = np_utils.to_categorical(y).astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create keras model\n",
    "batch_size = 512\n",
    "nb_epoch = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char_list), 64, input_length=maxLen, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(64, activation=\"tanh\", return_sequences=False, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\")))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels_list)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='char_lstm_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model.load_weights('char_lstm_keras_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80082 samples, validate on 8898 samples\n",
      "Epoch 1/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 4.3422Epoch 00000: val_loss improved from inf to 3.95216, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 165s - loss: 4.3410 - val_loss: 3.9522\n",
      "Epoch 2/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 3.6974Epoch 00001: val_loss improved from 3.95216 to 3.35652, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 163s - loss: 3.6965 - val_loss: 3.3565\n",
      "Epoch 3/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 3.2397Epoch 00002: val_loss improved from 3.35652 to 2.93774, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 162s - loss: 3.2389 - val_loss: 2.9377\n",
      "Epoch 4/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 2.8846Epoch 00003: val_loss improved from 2.93774 to 2.60893, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 162s - loss: 2.8843 - val_loss: 2.6089\n",
      "Epoch 5/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 2.5987Epoch 00004: val_loss improved from 2.60893 to 2.30819, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 162s - loss: 2.5978 - val_loss: 2.3082\n",
      "Epoch 6/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 2.3597Epoch 00005: val_loss improved from 2.30819 to 2.15431, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 162s - loss: 2.3593 - val_loss: 2.1543\n",
      "Epoch 7/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 2.1877Epoch 00006: val_loss improved from 2.15431 to 1.95581, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 163s - loss: 2.1872 - val_loss: 1.9558\n",
      "Epoch 8/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 2.0255Epoch 00007: val_loss improved from 1.95581 to 1.79230, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 163s - loss: 2.0252 - val_loss: 1.7923\n",
      "Epoch 9/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.8974Epoch 00008: val_loss improved from 1.79230 to 1.68214, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 163s - loss: 1.8975 - val_loss: 1.6821\n",
      "Epoch 10/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.8119Epoch 00009: val_loss improved from 1.68214 to 1.59369, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 163s - loss: 1.8117 - val_loss: 1.5937\n",
      "Epoch 11/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.7262Epoch 00010: val_loss improved from 1.59369 to 1.51978, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 163s - loss: 1.7260 - val_loss: 1.5198\n",
      "Epoch 12/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.6292Epoch 00011: val_loss improved from 1.51978 to 1.46506, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 163s - loss: 1.6295 - val_loss: 1.4651\n",
      "Epoch 13/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.5709Epoch 00012: val_loss improved from 1.46506 to 1.39176, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 164s - loss: 1.5706 - val_loss: 1.3918\n",
      "Epoch 14/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.5058Epoch 00013: val_loss improved from 1.39176 to 1.33750, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 164s - loss: 1.5051 - val_loss: 1.3375\n",
      "Epoch 15/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.4584Epoch 00014: val_loss improved from 1.33750 to 1.31102, saving model to char_lstm_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 163s - loss: 1.4587 - val_loss: 1.3110\n",
      "Epoch 16/200\n",
      "50176/80082 [=================>............] - ETA: 59s - loss: 1.4304"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-07cdcc341199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           callbacks=[early_stopping, checkpointer])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train model on test data\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('char_lstm_keras_weights.hdf5')\n",
    "preds = model.predict_classes(X_test, batch_size=64, verbose=0)\n",
    "\n",
    "print('')\n",
    "print(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))\n",
    "print('')\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm=confusion_matrix(np.argmax(y_test, axis=1), preds)\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels_list))\n",
    "plt.xticks(tick_marks, labels_list, rotation=45)\n",
    "plt.yticks(tick_marks, labels_list)\n",
    "thresh = cm.max() / 2.\n",
    "#for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#    plt.text(j, i, cm[i, j],horizontalalignment=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#plt.tight_layout()\n",
    "print(thresh)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create keras model\n",
    "batch_size = 512\n",
    "nb_epoch = 200\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(char_list), 64, input_length=maxLen, mask_zero=True))\n",
    "model2.add(Bidirectional(LSTM(64, activation=\"tanh\", return_sequences=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\")))\n",
    "#model2.add(Dropout(0.5))\n",
    "model2.add(LSTM(64, activation=\"tanh\", return_sequences=False, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\"))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(len(labels_list)))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='char_lstm2_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model2.load_weights('char_lstm2_keras_weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80082 samples, validate on 8898 samples\n",
      "Epoch 1/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 4.4492Epoch 00000: val_loss improved from inf to 4.24043, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 4.4489 - val_loss: 4.2404\n",
      "Epoch 2/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 4.0787Epoch 00001: val_loss improved from 4.24043 to 3.82922, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 4.0786 - val_loss: 3.8292\n",
      "Epoch 3/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 3.7426Epoch 00002: val_loss improved from 3.82922 to 3.51411, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 3.7422 - val_loss: 3.5141\n",
      "Epoch 4/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 3.4448Epoch 00003: val_loss improved from 3.51411 to 3.19975, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 3.4445 - val_loss: 3.1998\n",
      "Epoch 5/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 3.1300Epoch 00004: val_loss improved from 3.19975 to 2.83900, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 3.1297 - val_loss: 2.8390\n",
      "Epoch 6/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 2.7935Epoch 00005: val_loss improved from 2.83900 to 2.52976, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 2.7933 - val_loss: 2.5298\n",
      "Epoch 7/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 2.4995Epoch 00006: val_loss improved from 2.52976 to 2.21743, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 2.4990 - val_loss: 2.2174\n",
      "Epoch 8/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 2.2479Epoch 00007: val_loss improved from 2.21743 to 1.99716, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 282s - loss: 2.2473 - val_loss: 1.9972\n",
      "Epoch 9/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 2.0538Epoch 00008: val_loss improved from 1.99716 to 1.80015, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 282s - loss: 2.0534 - val_loss: 1.8002\n",
      "Epoch 10/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.8956Epoch 00009: val_loss improved from 1.80015 to 1.66010, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 282s - loss: 1.8952 - val_loss: 1.6601\n",
      "Epoch 11/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.7414Epoch 00010: val_loss improved from 1.66010 to 1.55260, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 282s - loss: 1.7407 - val_loss: 1.5526\n",
      "Epoch 12/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.6202Epoch 00011: val_loss improved from 1.55260 to 1.42368, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 1.6205 - val_loss: 1.4237\n",
      "Epoch 13/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.5140Epoch 00012: val_loss improved from 1.42368 to 1.34189, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 1.5136 - val_loss: 1.3419\n",
      "Epoch 14/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.4264Epoch 00013: val_loss improved from 1.34189 to 1.26344, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 1.4259 - val_loss: 1.2634\n",
      "Epoch 15/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.3505Epoch 00014: val_loss improved from 1.26344 to 1.18824, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 1.3505 - val_loss: 1.1882\n",
      "Epoch 16/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.2881Epoch 00015: val_loss improved from 1.18824 to 1.13386, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 282s - loss: 1.2880 - val_loss: 1.1339\n",
      "Epoch 17/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.2237Epoch 00016: val_loss improved from 1.13386 to 1.10655, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 282s - loss: 1.2236 - val_loss: 1.1065\n",
      "Epoch 18/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.1730Epoch 00017: val_loss improved from 1.10655 to 1.04446, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 282s - loss: 1.1730 - val_loss: 1.0445\n",
      "Epoch 19/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.1291Epoch 00018: val_loss improved from 1.04446 to 1.00796, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 282s - loss: 1.1288 - val_loss: 1.0080\n",
      "Epoch 20/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.0853Epoch 00019: val_loss improved from 1.00796 to 0.97640, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 282s - loss: 1.0851 - val_loss: 0.9764\n",
      "Epoch 21/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.0451Epoch 00020: val_loss improved from 0.97640 to 0.94484, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 1.0448 - val_loss: 0.9448\n",
      "Epoch 22/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 1.0022Epoch 00021: val_loss improved from 0.94484 to 0.90582, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 1.0021 - val_loss: 0.9058\n",
      "Epoch 23/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.9671Epoch 00022: val_loss improved from 0.90582 to 0.87382, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 0.9672 - val_loss: 0.8738\n",
      "Epoch 24/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.9350Epoch 00023: val_loss improved from 0.87382 to 0.86485, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 0.9350 - val_loss: 0.8649\n",
      "Epoch 25/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.9055Epoch 00024: val_loss improved from 0.86485 to 0.84024, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 0.9056 - val_loss: 0.8402\n",
      "Epoch 26/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.8858Epoch 00025: val_loss improved from 0.84024 to 0.82179, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 0.8855 - val_loss: 0.8218\n",
      "Epoch 27/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.8530Epoch 00026: val_loss improved from 0.82179 to 0.80530, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 0.8537 - val_loss: 0.8053\n",
      "Epoch 28/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.8300Epoch 00027: val_loss improved from 0.80530 to 0.78672, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 0.8303 - val_loss: 0.7867\n",
      "Epoch 29/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.8115Epoch 00028: val_loss improved from 0.78672 to 0.76627, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 283s - loss: 0.8110 - val_loss: 0.7663\n",
      "Epoch 30/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.7787Epoch 00029: val_loss improved from 0.76627 to 0.75281, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.7787 - val_loss: 0.7528\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.7569Epoch 00030: val_loss improved from 0.75281 to 0.75086, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.7571 - val_loss: 0.7509\n",
      "Epoch 32/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.7456Epoch 00031: val_loss improved from 0.75086 to 0.72043, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.7457 - val_loss: 0.7204\n",
      "Epoch 33/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.7232Epoch 00032: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.7229 - val_loss: 0.7255\n",
      "Epoch 34/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.7029Epoch 00033: val_loss improved from 0.72043 to 0.70272, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.7030 - val_loss: 0.7027\n",
      "Epoch 35/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.6914Epoch 00034: val_loss improved from 0.70272 to 0.69821, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.6913 - val_loss: 0.6982\n",
      "Epoch 36/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.6813Epoch 00035: val_loss improved from 0.69821 to 0.68789, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.6817 - val_loss: 0.6879\n",
      "Epoch 37/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.6598Epoch 00036: val_loss improved from 0.68789 to 0.67386, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.6598 - val_loss: 0.6739\n",
      "Epoch 38/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.6452Epoch 00037: val_loss improved from 0.67386 to 0.66637, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.6450 - val_loss: 0.6664\n",
      "Epoch 39/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.6224Epoch 00038: val_loss improved from 0.66637 to 0.65550, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.6225 - val_loss: 0.6555\n",
      "Epoch 40/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.6158Epoch 00039: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.6159 - val_loss: 0.6588\n",
      "Epoch 41/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.6148Epoch 00040: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.6148 - val_loss: 0.6742\n",
      "Epoch 42/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5933Epoch 00041: val_loss improved from 0.65550 to 0.64072, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.5934 - val_loss: 0.6407\n",
      "Epoch 43/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5857Epoch 00042: val_loss improved from 0.64072 to 0.63637, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.5858 - val_loss: 0.6364\n",
      "Epoch 44/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5635Epoch 00043: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.5637 - val_loss: 0.6412\n",
      "Epoch 45/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5479Epoch 00044: val_loss improved from 0.63637 to 0.61945, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.5479 - val_loss: 0.6195\n",
      "Epoch 46/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5526Epoch 00045: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.5525 - val_loss: 0.6358\n",
      "Epoch 47/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5333Epoch 00046: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.5333 - val_loss: 0.6231\n",
      "Epoch 48/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5453Epoch 00047: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.5453 - val_loss: 0.6320\n",
      "Epoch 49/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5217Epoch 00048: val_loss improved from 0.61945 to 0.61296, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.5218 - val_loss: 0.6130\n",
      "Epoch 50/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5012Epoch 00049: val_loss improved from 0.61296 to 0.61144, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.5013 - val_loss: 0.6114\n",
      "Epoch 51/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4987Epoch 00050: val_loss did not improve\n",
      "80082/80082 [==============================] - 285s - loss: 0.4985 - val_loss: 0.6277\n",
      "Epoch 52/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4931Epoch 00051: val_loss improved from 0.61144 to 0.60376, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.4932 - val_loss: 0.6038\n",
      "Epoch 53/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4808Epoch 00052: val_loss improved from 0.60376 to 0.60225, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.4810 - val_loss: 0.6022\n",
      "Epoch 54/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4659Epoch 00053: val_loss did not improve\n",
      "80082/80082 [==============================] - 285s - loss: 0.4660 - val_loss: 0.6092\n",
      "Epoch 55/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4637Epoch 00054: val_loss improved from 0.60225 to 0.60000, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.4636 - val_loss: 0.6000\n",
      "Epoch 56/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4503Epoch 00055: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.4502 - val_loss: 0.6052\n",
      "Epoch 57/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4538Epoch 00056: val_loss did not improve\n",
      "80082/80082 [==============================] - 283s - loss: 0.4537 - val_loss: 0.6035\n",
      "Epoch 58/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4468Epoch 00057: val_loss did not improve\n",
      "80082/80082 [==============================] - 283s - loss: 0.4469 - val_loss: 0.6058\n",
      "Epoch 59/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4537Epoch 00058: val_loss did not improve\n",
      "80082/80082 [==============================] - 283s - loss: 0.4540 - val_loss: 0.6694\n",
      "Epoch 60/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.5032Epoch 00059: val_loss improved from 0.60000 to 0.59359, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.5037 - val_loss: 0.5936\n",
      "Epoch 61/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4299Epoch 00060: val_loss improved from 0.59359 to 0.59197, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 285s - loss: 0.4297 - val_loss: 0.5920\n",
      "Epoch 62/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4174Epoch 00061: val_loss improved from 0.59197 to 0.58158, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.4171 - val_loss: 0.5816\n",
      "Epoch 63/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4176Epoch 00062: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.4175 - val_loss: 0.5905\n",
      "Epoch 64/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4011Epoch 00063: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80082/80082 [==============================] - 284s - loss: 0.4013 - val_loss: 0.5857\n",
      "Epoch 65/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4067Epoch 00064: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.4067 - val_loss: 0.5878\n",
      "Epoch 66/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4005Epoch 00065: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.4006 - val_loss: 0.5869\n",
      "Epoch 67/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.3945Epoch 00066: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.3944 - val_loss: 0.5870\n",
      "Epoch 68/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.4001Epoch 00067: val_loss improved from 0.58158 to 0.57497, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 284s - loss: 0.4002 - val_loss: 0.5750\n",
      "Epoch 69/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.3766Epoch 00068: val_loss improved from 0.57497 to 0.57158, saving model to char_lstm2_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 285s - loss: 0.3769 - val_loss: 0.5716\n",
      "Epoch 70/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.3657Epoch 00069: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.3658 - val_loss: 0.5873\n",
      "Epoch 71/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.3744Epoch 00070: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.3747 - val_loss: 0.5798\n",
      "Epoch 72/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.3779Epoch 00071: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.3779 - val_loss: 0.5975\n",
      "Epoch 73/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.3748Epoch 00072: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.3747 - val_loss: 0.5802\n",
      "Epoch 74/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.3723Epoch 00073: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.3726 - val_loss: 0.5906\n",
      "Epoch 75/200\n",
      "79872/80082 [============================>.] - ETA: 0s - loss: 0.3707Epoch 00074: val_loss did not improve\n",
      "80082/80082 [==============================] - 284s - loss: 0.3706 - val_loss: 0.5893\n",
      "Epoch 00074: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae88a11a20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model on test data\n",
    "model2.fit(X_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "          General Dynamic Performance       0.79      0.84      0.81       551\n",
      "           General Rear Entertainment       0.71      0.86      0.78        83\n",
      "                      Rear Visibility       0.90      0.90      0.90       281\n",
      "    Tilt / Telescoping Steering Wheel       0.81      0.79      0.80        68\n",
      "             Environmentally Friendly       0.78      0.76      0.77       134\n",
      "                        General Seats       0.72      0.80      0.76       506\n",
      "                           Headlights       0.94      0.98      0.96       395\n",
      "         Appearance of Steering Wheel       0.33      0.19      0.24        21\n",
      "          Defrost / Defog Performance       0.94      0.89      0.91       164\n",
      "                         Fuel Economy       0.96      0.98      0.97      1464\n",
      "             General Exterior Styling       0.86      0.80      0.83       172\n",
      "                       General Access       0.83      0.86      0.85       421\n",
      "                        NAVI Controls       0.90      0.84      0.87       206\n",
      "                      Radio Reception       0.94      0.52      0.67        33\n",
      "                          Rear Camera       0.99      1.00      0.99       613\n",
      "           Wiper / Washer Performance       0.84      0.94      0.89       465\n",
      "        Appearance of Gauges/Controls       0.89      0.90      0.89       428\n",
      "                          Brake Noise       0.78      0.64      0.70        22\n",
      "                 Remote Keyless Entry       0.90      0.90      0.90       183\n",
      "                  Manual Transmission       0.91      0.53      0.67        76\n",
      "                       Center Console       0.92      0.96      0.94       675\n",
      "                         Rear Styling       0.75      0.60      0.67        25\n",
      "                Steering Wheel Heater       0.86      0.91      0.88        55\n",
      "                       Turning Radius       0.87      0.87      0.87        67\n",
      "        Quietness During Acceleration       0.88      0.86      0.87       227\n",
      "                 Side Profile Styling       0.72      0.59      0.65        87\n",
      "                       Cargo Capacity       0.88      0.92      0.90      1023\n",
      "                            Glove Box       0.92      0.94      0.93       220\n",
      "                        Passing Power       0.96      0.90      0.93       135\n",
      "                            Roof Rack       0.97      1.00      0.98        28\n",
      "Power Seat / Memory Seat Availability       0.80      0.85      0.82       142\n",
      "                                Other       0.50      0.60      0.54       363\n",
      "        Materials Collect Soil / Lint       0.57      0.40      0.47        40\n",
      "                      Front Roominess       0.92      0.89      0.90       201\n",
      "                   Acceleration Sound       0.90      0.78      0.84        46\n",
      "                    Seats Wear Easily       0.00      0.00      0.00        36\n",
      "                                  DQR       0.73      0.77      0.75       397\n",
      "             Steering Controllability       0.86      0.87      0.86       127\n",
      "            Interior Material Quality       0.78      0.62      0.69       124\n",
      "             Acceleration from a Stop       0.92      0.92      0.92      1086\n",
      "  12V / USB / Aux Location / Quantity       0.84      0.55      0.67        85\n",
      "          General Visibility / Safety       0.83      0.90      0.86       381\n",
      "                  Appearance of Paint       0.97      0.99      0.98       423\n",
      "                          Cargo Cover       1.00      0.69      0.82        45\n",
      "                         General NAVI       0.89      0.90      0.90       495\n",
      "                        Trunk Release       0.84      0.66      0.74        41\n",
      "             Audio Control Visibility       0.83      0.21      0.33        24\n",
      "                Tire Pressure Monitor       0.98      0.93      0.95        54\n",
      "                Seat Material Quality       0.79      0.84      0.81       440\n",
      "                           Road Noise       0.88      0.92      0.90       386\n",
      "             Location of Pedals / PKB       0.83      0.72      0.77        80\n",
      "               General Fit and Finish       0.51      0.29      0.37       113\n",
      "                   Garage Door Opener       0.93      0.93      0.93        67\n",
      "              Transmission Smoothness       0.86      0.87      0.86       575\n",
      "                     Interior Storage       0.89      0.86      0.87       588\n",
      "                        Seat Controls       0.91      0.85      0.88       293\n",
      "                   Front Seat Comfort       0.80      0.84      0.82       574\n",
      "                  Door Handle / Locks       0.92      0.88      0.90       266\n",
      "                     Appearance of IP       0.90      0.92      0.91       595\n",
      "                              Mirrors       0.86      0.86      0.86       373\n",
      "                         Ease to Park       0.94      0.88      0.91       522\n",
      "      Stability in Adverse Conditions       0.88      0.90      0.89       748\n",
      "                    Seat Belt Comfort       0.86      0.94      0.90       183\n",
      "                Brake Controllability       0.77      0.82      0.79       240\n",
      "                       HVAC Quietness       0.73      0.81      0.77        47\n",
      "          Door Closing Effort / Sound       0.85      0.74      0.79        78\n",
      "                 Seat Heater / Cooler       0.95      0.97      0.96       466\n",
      "                              Compass       1.00      0.97      0.99        78\n",
      "                        Driving Range       0.80      0.84      0.82       157\n",
      "               Bluetooth Connectivity       0.95      0.97      0.96       565\n",
      "                         Tow Capacity       1.00      0.73      0.84        63\n",
      "                 3rd Row Seat Comfort       0.91      0.94      0.93       208\n",
      "                   General Powertrain       0.79      0.83      0.81       307\n",
      "   Ease to Use Controls While Driving       0.82      0.83      0.83       288\n",
      "                     Rear Cup Holders       0.83      0.64      0.72        39\n",
      "                       Rear Roominess       0.64      0.44      0.52        85\n",
      "                  Audio Sound Quality       0.72      0.62      0.67       130\n",
      "              Wiper / Washer Controls       0.44      0.22      0.29        92\n",
      "                  Perceived Roominess       0.72      0.73      0.73       140\n",
      "            Seats Collect Soil / Lint       0.71      0.66      0.68        77\n",
      "               General Riding Comfort       0.95      0.93      0.94      1261\n",
      "         Passenger Seat Adjustability       0.83      0.78      0.80        55\n",
      "                         Fun to Drive       0.76      0.71      0.74        63\n",
      "                 Appearance of Wheels       0.90      0.78      0.84        78\n",
      "                    Front Cup Holders       0.94      0.92      0.93       479\n",
      "                      Abnormal Noises       0.66      0.57      0.61       173\n",
      "                         General HVAC       0.55      0.58      0.57       109\n",
      "                 2nd Row Seat Comfort       0.81      0.86      0.83       537\n",
      "              Vibration while Driving       0.70      0.80      0.75        35\n",
      "                Transmission Response       0.85      0.76      0.80       158\n",
      "                  Remote Engine Start       0.96      0.90      0.93        99\n",
      "        Heating / Cooling Performance       0.62      0.54      0.58        85\n",
      "                       Cargo Lighting       0.76      0.89      0.82        18\n",
      "               Lane Change Visibility       0.89      0.92      0.90       557\n",
      "                             Arm Rest       0.92      0.90      0.91       182\n",
      "            Driver Seat Adjustability       0.51      0.45      0.48        51\n",
      "                       Lumbar Support       1.00      1.00      1.00       237\n",
      "                  Gearshift Operation       0.64      0.76      0.69       120\n",
      "                   Vent Adjustability       0.88      0.87      0.88       157\n",
      "        Appearance of Interior Lights       0.84      0.84      0.84        97\n",
      "              Bluetooth Sound Quality       1.00      0.56      0.72        34\n",
      "                 Harshness Over Bumps       0.91      0.80      0.85        51\n",
      "                        Front Styling       0.70      0.71      0.70        65\n",
      "                       Cruise Control       0.95      0.90      0.93       185\n",
      "                    Smell of Interior       0.66      0.49      0.56        39\n",
      "              Steering Wheel Controls       0.89      0.90      0.90       198\n",
      "                           Wind Noise       0.92      0.78      0.85        78\n",
      "                Ease of Loading Cargo       0.79      0.79      0.79       329\n",
      "                       Audio Controls       0.80      0.75      0.77       201\n",
      "                     Front Visibility       0.87      0.90      0.88       513\n",
      "           Audio Format Compatibility       0.89      0.93      0.91       290\n",
      "                       Brake Capacity       0.78      0.53      0.63       105\n",
      "               Materials Scuff Easily       0.65      0.50      0.57       112\n",
      "                    Squeak and Rattle       0.93      0.92      0.93       105\n",
      "                  Seating Flexibility       0.72      0.66      0.69       175\n",
      "                 Satellite / HD Radio       0.88      0.91      0.89       252\n",
      "                        General Audio       0.71      0.80      0.75       312\n",
      "                           Pickup Box       0.91      0.86      0.88       135\n",
      "                           Sun Visors       0.97      0.96      0.97       130\n",
      "\n",
      "                          avg / total       0.86      0.86      0.86     29661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2.load_weights('char_lstm2_keras_weights.hdf5')\n",
    "preds = model2.predict_classes(X_test, batch_size=64, verbose=0)\n",
    "\n",
    "print('')\n",
    "print(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))\n",
    "#print('')\n",
    "#print(confusion_matrix(np.argmax(y_test, axis=1), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create keras model\n",
    "batch_size = 512\n",
    "nb_epoch = 200\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(len(char_list), 64, input_length=maxLen, mask_zero=True))\n",
    "model3.add(Bidirectional(LSTM(64, activation=\"tanh\", return_sequences=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\")))\n",
    "#model2.add(Dropout(0.5))\n",
    "model3.add(LSTM(64, activation=\"tanh\", return_sequences=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\"))\n",
    "#model3.add(Dropout(0.5))\n",
    "model3.add(LSTM(64, activation=\"tanh\", return_sequences=False, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\"))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(len(labels_list)))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='char_lstm3_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model2.load_weights('char_lstm3_keras_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80082 samples, validate on 8898 samples\n",
      "Epoch 1/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 4.4622Epoch 00000: val_loss improved from inf to 4.31479, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 404s - loss: 4.4620 - val_loss: 4.3148\n",
      "Epoch 2/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 4.1913Epoch 00001: val_loss improved from 4.31479 to 4.04185, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 401s - loss: 4.1909 - val_loss: 4.0418\n",
      "Epoch 3/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 4.0283Epoch 00002: val_loss improved from 4.04185 to 3.89840, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 401s - loss: 4.0280 - val_loss: 3.8984\n",
      "Epoch 4/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 3.8492Epoch 00003: val_loss improved from 3.89840 to 3.68003, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 401s - loss: 3.8488 - val_loss: 3.6800\n",
      "Epoch 5/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 3.5435Epoch 00004: val_loss improved from 3.68003 to 3.25636, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 3.5433 - val_loss: 3.2564\n",
      "Epoch 6/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 3.1584Epoch 00005: val_loss improved from 3.25636 to 2.89013, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 3.1581 - val_loss: 2.8901\n",
      "Epoch 7/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 2.8552Epoch 00006: val_loss improved from 2.89013 to 2.59473, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 2.8552 - val_loss: 2.5947\n",
      "Epoch 8/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 2.6179Epoch 00007: val_loss improved from 2.59473 to 2.34370, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 2.6169 - val_loss: 2.3437\n",
      "Epoch 9/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 2.3570Epoch 00008: val_loss improved from 2.34370 to 2.12231, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 2.3565 - val_loss: 2.1223\n",
      "Epoch 10/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 2.1403Epoch 00009: val_loss improved from 2.12231 to 1.88014, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 2.1403 - val_loss: 1.8801\n",
      "Epoch 11/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.9379Epoch 00010: val_loss improved from 1.88014 to 1.70105, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 1.9374 - val_loss: 1.7011\n",
      "Epoch 12/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.7821Epoch 00011: val_loss improved from 1.70105 to 1.57369, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 1.7818 - val_loss: 1.5737\n",
      "Epoch 13/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.6379Epoch 00012: val_loss improved from 1.57369 to 1.46711, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 404s - loss: 1.6371 - val_loss: 1.4671\n",
      "Epoch 14/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.5280Epoch 00013: val_loss improved from 1.46711 to 1.35432, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 1.5277 - val_loss: 1.3543\n",
      "Epoch 15/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.4258Epoch 00014: val_loss improved from 1.35432 to 1.28034, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 1.4251 - val_loss: 1.2803\n",
      "Epoch 16/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.3614Epoch 00015: val_loss improved from 1.28034 to 1.19670, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 1.3614 - val_loss: 1.1967\n",
      "Epoch 17/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.2746Epoch 00016: val_loss improved from 1.19670 to 1.14930, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 1.2744 - val_loss: 1.1493\n",
      "Epoch 18/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.2155Epoch 00017: val_loss improved from 1.14930 to 1.12753, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 1.2154 - val_loss: 1.1275\n",
      "Epoch 19/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.1629Epoch 00018: val_loss improved from 1.12753 to 1.05209, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 1.1626 - val_loss: 1.0521\n",
      "Epoch 20/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.1218Epoch 00019: val_loss improved from 1.05209 to 1.03899, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 1.1222 - val_loss: 1.0390\n",
      "Epoch 21/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.0961Epoch 00020: val_loss did not improve\n",
      "80082/80082 [==============================] - 401s - loss: 1.0962 - val_loss: 1.0737\n",
      "Epoch 22/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 1.0463Epoch 00021: val_loss improved from 1.03899 to 0.95503, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 1.0461 - val_loss: 0.9550\n",
      "Epoch 23/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.9885Epoch 00022: val_loss improved from 0.95503 to 0.91356, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.9881 - val_loss: 0.9136\n",
      "Epoch 24/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.9534Epoch 00023: val_loss improved from 0.91356 to 0.90081, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.9532 - val_loss: 0.9008\n",
      "Epoch 25/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.9172Epoch 00024: val_loss improved from 0.90081 to 0.86114, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.9170 - val_loss: 0.8611\n",
      "Epoch 26/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.8906Epoch 00025: val_loss improved from 0.86114 to 0.85575, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.8903 - val_loss: 0.8558\n",
      "Epoch 27/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.8610Epoch 00026: val_loss improved from 0.85575 to 0.83033, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.8609 - val_loss: 0.8303\n",
      "Epoch 28/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.8309Epoch 00027: val_loss improved from 0.83033 to 0.80152, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.8313 - val_loss: 0.8015\n",
      "Epoch 29/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.8044Epoch 00028: val_loss improved from 0.80152 to 0.78274, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.8046 - val_loss: 0.7827\n",
      "Epoch 30/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.7810Epoch 00029: val_loss improved from 0.78274 to 0.76124, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.7809 - val_loss: 0.7612\n",
      "Epoch 31/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.7556Epoch 00030: val_loss improved from 0.76124 to 0.74225, saving model to char_lstm3_keras_weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80082/80082 [==============================] - 403s - loss: 0.7561 - val_loss: 0.7423\n",
      "Epoch 32/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.7351Epoch 00031: val_loss improved from 0.74225 to 0.74162, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.7350 - val_loss: 0.7416\n",
      "Epoch 33/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.7116Epoch 00032: val_loss improved from 0.74162 to 0.72863, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.7116 - val_loss: 0.7286\n",
      "Epoch 34/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.6847Epoch 00033: val_loss improved from 0.72863 to 0.70879, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.6845 - val_loss: 0.7088\n",
      "Epoch 35/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.6659Epoch 00034: val_loss did not improve\n",
      "80082/80082 [==============================] - 403s - loss: 0.6661 - val_loss: 0.7161\n",
      "Epoch 36/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.6536Epoch 00035: val_loss improved from 0.70879 to 0.69973, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.6534 - val_loss: 0.6997\n",
      "Epoch 37/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.6337Epoch 00036: val_loss improved from 0.69973 to 0.68487, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.6337 - val_loss: 0.6849\n",
      "Epoch 38/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.6190Epoch 00037: val_loss improved from 0.68487 to 0.67591, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.6192 - val_loss: 0.6759\n",
      "Epoch 39/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.6031Epoch 00038: val_loss improved from 0.67591 to 0.66654, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 404s - loss: 0.6034 - val_loss: 0.6665\n",
      "Epoch 40/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.6104Epoch 00039: val_loss improved from 0.66654 to 0.66390, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.6102 - val_loss: 0.6639\n",
      "Epoch 41/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.5904Epoch 00040: val_loss did not improve\n",
      "80082/80082 [==============================] - 403s - loss: 0.5902 - val_loss: 0.6764\n",
      "Epoch 42/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.5628Epoch 00041: val_loss did not improve\n",
      "80082/80082 [==============================] - 402s - loss: 0.5629 - val_loss: 0.6640\n",
      "Epoch 43/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.5492Epoch 00042: val_loss improved from 0.66390 to 0.64816, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.5489 - val_loss: 0.6482\n",
      "Epoch 44/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.5296Epoch 00043: val_loss improved from 0.64816 to 0.63461, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 404s - loss: 0.5299 - val_loss: 0.6346\n",
      "Epoch 45/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.5390Epoch 00044: val_loss improved from 0.63461 to 0.63334, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.5390 - val_loss: 0.6333\n",
      "Epoch 46/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.5071Epoch 00045: val_loss improved from 0.63334 to 0.62867, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.5069 - val_loss: 0.6287\n",
      "Epoch 47/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.5006Epoch 00046: val_loss did not improve\n",
      "80082/80082 [==============================] - 403s - loss: 0.5009 - val_loss: 0.6333\n",
      "Epoch 48/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4852Epoch 00047: val_loss improved from 0.62867 to 0.62362, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 411s - loss: 0.4848 - val_loss: 0.6236\n",
      "Epoch 49/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4778Epoch 00048: val_loss did not improve\n",
      "80082/80082 [==============================] - 417s - loss: 0.4778 - val_loss: 0.6309\n",
      "Epoch 50/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4630Epoch 00049: val_loss did not improve\n",
      "80082/80082 [==============================] - 414s - loss: 0.4633 - val_loss: 0.6286\n",
      "Epoch 51/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4580Epoch 00050: val_loss did not improve\n",
      "80082/80082 [==============================] - 411s - loss: 0.4581 - val_loss: 0.6309\n",
      "Epoch 52/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4481Epoch 00051: val_loss improved from 0.62362 to 0.62091, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 403s - loss: 0.4478 - val_loss: 0.6209\n",
      "Epoch 53/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4360Epoch 00052: val_loss improved from 0.62091 to 0.61413, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.4359 - val_loss: 0.6141\n",
      "Epoch 54/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4195Epoch 00053: val_loss improved from 0.61413 to 0.60356, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.4197 - val_loss: 0.6036\n",
      "Epoch 55/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4180Epoch 00054: val_loss did not improve\n",
      "80082/80082 [==============================] - 402s - loss: 0.4178 - val_loss: 0.6196\n",
      "Epoch 56/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4138Epoch 00055: val_loss did not improve\n",
      "80082/80082 [==============================] - 402s - loss: 0.4137 - val_loss: 0.6352\n",
      "Epoch 57/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.4208Epoch 00056: val_loss did not improve\n",
      "80082/80082 [==============================] - 406s - loss: 0.4208 - val_loss: 0.6116\n",
      "Epoch 58/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3938Epoch 00057: val_loss did not improve\n",
      "80082/80082 [==============================] - 403s - loss: 0.3936 - val_loss: 0.6091\n",
      "Epoch 59/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3877Epoch 00058: val_loss improved from 0.60356 to 0.60056, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 402s - loss: 0.3877 - val_loss: 0.6006\n",
      "Epoch 60/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3832Epoch 00059: val_loss did not improve\n",
      "80082/80082 [==============================] - 402s - loss: 0.3830 - val_loss: 0.6074\n",
      "Epoch 61/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3790Epoch 00060: val_loss did not improve\n",
      "80082/80082 [==============================] - 405s - loss: 0.3787 - val_loss: 0.6016\n",
      "Epoch 62/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3662Epoch 00061: val_loss did not improve\n",
      "80082/80082 [==============================] - 401s - loss: 0.3662 - val_loss: 0.6108\n",
      "Epoch 63/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3741Epoch 00062: val_loss did not improve\n",
      "80082/80082 [==============================] - 403s - loss: 0.3744 - val_loss: 0.6056\n",
      "Epoch 64/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3633Epoch 00063: val_loss did not improve\n",
      "80082/80082 [==============================] - 402s - loss: 0.3632 - val_loss: 0.6151\n",
      "Epoch 65/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3575Epoch 00064: val_loss improved from 0.60056 to 0.59926, saving model to char_lstm3_keras_weights.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80082/80082 [==============================] - 402s - loss: 0.3575 - val_loss: 0.5993\n",
      "Epoch 66/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3391Epoch 00065: val_loss did not improve\n",
      "80082/80082 [==============================] - 402s - loss: 0.3393 - val_loss: 0.6124\n",
      "Epoch 67/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3393Epoch 00066: val_loss did not improve\n",
      "80082/80082 [==============================] - 402s - loss: 0.3394 - val_loss: 0.6194\n",
      "Epoch 68/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3345Epoch 00067: val_loss did not improve\n",
      "80082/80082 [==============================] - 402s - loss: 0.3345 - val_loss: 0.6002\n",
      "Epoch 69/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3314Epoch 00068: val_loss did not improve\n",
      "80082/80082 [==============================] - 403s - loss: 0.3312 - val_loss: 0.6114\n",
      "Epoch 70/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 0.3222Epoch 00069: val_loss did not improve\n",
      "80082/80082 [==============================] - 404s - loss: 0.3225 - val_loss: 0.6152\n",
      "Epoch 71/200\n",
      " 7680/80082 [=>............................] - ETA: 351s - loss: 0.3171"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b2aceee8ad28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           callbacks=[early_stopping, checkpointer])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train model on test data\n",
    "model3.fit(X_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3.load_weights('char_lstm3_keras_weights.hdf5')\n",
    "preds = model2.predict_classes(X_test, batch_size=64, verbose=0)\n",
    "\n",
    "print('')\n",
    "print(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))\n",
    "#print('')\n",
    "#print(confusion_matrix(np.argmax(y_test, axis=1), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v=[]\n",
    "with codecs.open('Data/verbatims.csv', 'r', encoding='ascii', errors='ignore') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        v.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_real=[]\n",
    "vo=map(lambda x:x.upper().replace(\"'\",\"\").replace(\"\\\"\",\"\").replace(\"?\",\"\").replace(\"~\",\"\"),v)\n",
    "for n in vo:\n",
    "    X_real.append(verbatim_to_char_seq(n, char_indices, maxLen))\n",
    "    \n",
    "X_real = np.array(X_real).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = model2.predict_classes(X_real, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pArray = map(lambda x:indices_label[x],p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('Data/verbatimResults.csv', 'w', encoding='ascii', errors='ignore') as f:\n",
    "    wr = csv.writer(f, dialect='excel')\n",
    "    for item in pArray:\n",
    "        wr.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
