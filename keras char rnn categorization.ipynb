{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, LSTM, GRU, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import csv\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verbatim_to_char_seq(name, char_indices, maxLen):\n",
    "    name_chars = list(name)\n",
    "    name_chars_indices = list(map(lambda char: char_indices[char], name_chars))\n",
    "    return sequence.pad_sequences([name_chars_indices], maxLen)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import verbatims, put into two lists\n",
    "verbatims = []\n",
    "labels = []\n",
    "allowedChars = []\n",
    "with codecs.open('Data/Catagorization training data.csv', 'r', encoding='ascii', errors='ignore') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        temp = line[1].replace('\\n', ' ').replace('\\r', ' ').replace('<', ' ').replace('>', ' ').replace('*', ' ')\n",
    "        temp = temp.replace('%', ' ').replace('&', ' ').replace('#', ' ').replace('~', ' ').replace('@', ' ')\n",
    "        temp = temp.replace('=', ' ').replace('`', ' ').replace(';', ' ').replace('_', ' ').replace('+', ' ')\n",
    "        temp = (temp[:198] + '..') if len(temp) > 200 else temp\n",
    "        verbatims.append(temp)\n",
    "        labels.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_list = list(set(''.join(verbatims)))\n",
    "labels_list = list(set(labels))\n",
    "char_indices = dict((c, i) for i, c in enumerate(char_list))\n",
    "indices_char = dict((i, c) for i, c in enumerate(char_list))\n",
    "label_indices = dict((l, i) for i, l in enumerate(labels_list))\n",
    "indices_label = dict((i, l) for i, l in enumerate(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get rid of duplicates and blanks\n",
    "\n",
    "objs = []\n",
    "for obj in list(zip(verbatims, labels)):\n",
    "    if len(obj[0].strip()) != 0:\n",
    "        objs.append(obj)\n",
    "\n",
    "objs = list(set(objs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118640\n"
     ]
    }
   ],
   "source": [
    "#separate out verbatims and labels again\n",
    "verbatims = []\n",
    "labels = []\n",
    "\n",
    "for n, l in objs:\n",
    "    verbatims.append(n)\n",
    "    labels.append(l)\n",
    "    \n",
    "print(len(verbatims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#determine the maximum length of the verbatims\n",
    "maxLen = 0\n",
    "for v in verbatims:\n",
    "    if len(v) > maxLen:\n",
    "        maxLen = len(v)\n",
    "print(maxLen)\n",
    "\n",
    "#if the max length is < 50, pad verbatim\n",
    "#if maxLen < 50:\n",
    "#    maxLen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118640, 200) (118640, 119)\n"
     ]
    }
   ],
   "source": [
    "#create actual dataset to be fed into keras model\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for n, l in zip(verbatims, labels):\n",
    "    X.append(verbatim_to_char_seq(n, char_indices, maxLen))\n",
    "    y.append(label_indices[l])\n",
    "    \n",
    "X = np.array(X).astype(np.uint8)\n",
    "y=np.array(y)\n",
    "y = np_utils.to_categorical(np.array(y)).astype(np.bool)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump((char_list, labels_list, char_indices, indices_char, label_indices, indices_label, X, y), open('char_lstm_file.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "char_list, labels_list, char_indices, indices_char, label_indices, indices_label, X, y = pickle.load(open( 'char_lstm_file.pkl', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118640, 200) (118640, 119)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "#y = np_utils.to_categorical(y).astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create keras model\n",
    "batch_size = 512\n",
    "nb_epoch = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char_list), 64, input_length=maxLen, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(64, activation=\"tanh\", return_sequences=False, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\")))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels_list)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='char_lstm_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model.load_weights('char_lstm_keras_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model on test data\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('char_lstm_keras_weights.hdf5')\n",
    "preds = model.predict_classes(X_test, batch_size=64, verbose=0)\n",
    "\n",
    "print('')\n",
    "print(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))\n",
    "print('')\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm=confusion_matrix(np.argmax(y_test, axis=1), preds)\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels_list))\n",
    "plt.xticks(tick_marks, labels_list, rotation=45)\n",
    "plt.yticks(tick_marks, labels_list)\n",
    "thresh = cm.max() / 2.\n",
    "#for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#    plt.text(j, i, cm[i, j],horizontalalignment=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#plt.tight_layout()\n",
    "print(thresh)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create keras model\n",
    "batch_size = 512\n",
    "nb_epoch = 200\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(char_list), 64, input_length=maxLen, mask_zero=True))\n",
    "model2.add(Bidirectional(LSTM(64, activation=\"tanh\", return_sequences=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\")))\n",
    "#model2.add(Dropout(0.5))\n",
    "model2.add(LSTM(64, activation=\"tanh\", return_sequences=False, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\"))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(len(labels_list)))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='char_lstm2_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model2.load_weights('char_lstm2_keras_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model on test data\n",
    "model2.fit(X_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.load_weights('char_lstm2_keras_weights.hdf5')\n",
    "preds = model2.predict_classes(X_test, batch_size=512, verbose=0)\n",
    "\n",
    "print('')\n",
    "print(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))\n",
    "#print('')\n",
    "#print(confusion_matrix(np.argmax(y_test, axis=1), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create keras model\n",
    "batch_size = 512\n",
    "nb_epoch = 200\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(len(char_list), 64, input_length=maxLen, mask_zero=True))\n",
    "model3.add(Bidirectional(LSTM(64, activation=\"tanh\", return_sequences=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\")))\n",
    "model3.add(LSTM(64, activation=\"tanh\", return_sequences=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\"))\n",
    "model3.add(LSTM(64, activation=\"tanh\", return_sequences=False, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\"))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(len(labels_list)))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='char_lstm3_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model3.load_weights('char_lstm3_keras_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80082 samples, validate on 8898 samples\n",
      "Epoch 1/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 4.3888Epoch 00000: val_loss improved from inf to 4.19381, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 406s - loss: 4.3886 - val_loss: 4.1938\n",
      "Epoch 2/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 4.1858Epoch 00001: val_loss improved from 4.19381 to 4.10222, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 404s - loss: 4.1855 - val_loss: 4.1022\n",
      "Epoch 3/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 4.0810Epoch 00002: val_loss improved from 4.10222 to 3.99658, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 404s - loss: 4.0806 - val_loss: 3.9966\n",
      "Epoch 4/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 3.9524Epoch 00003: val_loss improved from 3.99658 to 3.81424, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 405s - loss: 3.9520 - val_loss: 3.8142\n",
      "Epoch 5/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 3.7911Epoch 00004: val_loss improved from 3.81424 to 3.65563, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 405s - loss: 3.7909 - val_loss: 3.6556\n",
      "Epoch 6/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 3.6105Epoch 00005: val_loss improved from 3.65563 to 3.47741, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 405s - loss: 3.6108 - val_loss: 3.4774\n",
      "Epoch 7/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 3.4174Epoch 00006: val_loss improved from 3.47741 to 3.27092, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 406s - loss: 3.4173 - val_loss: 3.2709\n",
      "Epoch 8/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 3.2217Epoch 00007: val_loss improved from 3.27092 to 3.15969, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 405s - loss: 3.2216 - val_loss: 3.1597\n",
      "Epoch 9/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 3.0192Epoch 00008: val_loss improved from 3.15969 to 2.83539, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 407s - loss: 3.0188 - val_loss: 2.8354\n",
      "Epoch 10/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 2.7996Epoch 00009: val_loss improved from 2.83539 to 2.59233, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 406s - loss: 2.7994 - val_loss: 2.5923\n",
      "Epoch 11/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 2.5831Epoch 00010: val_loss improved from 2.59233 to 2.37475, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 406s - loss: 2.5826 - val_loss: 2.3747\n",
      "Epoch 12/200\n",
      "79872/80082 [============================>.] - ETA: 1s - loss: 2.3959Epoch 00011: val_loss improved from 2.37475 to 2.19486, saving model to char_lstm3_keras_weights.hdf5\n",
      "80082/80082 [==============================] - 406s - loss: 2.3961 - val_loss: 2.1949\n",
      "Epoch 13/200\n",
      "11776/80082 [===>..........................] - ETA: 334s - loss: 2.2877"
     ]
    }
   ],
   "source": [
    "#train model on test data\n",
    "model3.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.load_weights('char_lstm3_keras_weights.hdf5')\n",
    "preds = model3.predict_classes(X_test, batch_size=512, verbose=0)\n",
    "\n",
    "print('')\n",
    "print(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create keras model\n",
    "batch_size = 512\n",
    "nb_epoch = 200\n",
    "activations=['elu','softplus','softsign','relu','tanh','sigmoid','hard_sigmoid','linear']\n",
    "models=[]\n",
    "for activation1 in activations:\n",
    "    for activation2 in activations:\n",
    "        model3 = Sequential()\n",
    "        model3.add(Embedding(len(char_list), 64, input_length=maxLen, mask_zero=True))\n",
    "        model3.add(Bidirectional(LSTM(64, activation=\"tanh\", return_sequences=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\")))\n",
    "        #model2.add(Dropout(0.5))\n",
    "        model3.add(LSTM(64, activation=\"tanh\", return_sequences=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\"))\n",
    "        #model3.add(Dropout(0.5))\n",
    "        model3.add(LSTM(64, activation=\"tanh\", return_sequences=False, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\"))\n",
    "        model3.add(Dropout(0.5))\n",
    "        model3.add(Dense(len(labels_list)))\n",
    "        model3.add(Activation('softmax'))\n",
    "        model3.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        models.append([activation1, activation2, model])\n",
    "        \n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "#checkpointer = ModelCheckpoint(filepath='char_lstm3_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "#model2.load_weights('char_lstm3_keras_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train models on test data\n",
    "for model in models:\n",
    "    m=model[2]\n",
    "    m.fit(X_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=nb_epoch,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping])\n",
    "    preds = m.predict_classes(X_test, batch_size=512, verbose=0)\n",
    "    print(model[0], model[1])\n",
    "    print(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3.load_weights('char_lstm3_keras_weights.hdf5')\n",
    "preds = model2.predict_classes(X_test, batch_size=64, verbose=0)\n",
    "\n",
    "print('')\n",
    "print(classification_report(np.argmax(y_test, axis=1), preds, target_names=labels_list))\n",
    "#print('')\n",
    "#print(confusion_matrix(np.argmax(y_test, axis=1), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v=[]\n",
    "with codecs.open('Data/verbatims.csv', 'r', encoding='ascii', errors='ignore') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        v.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_real=[]\n",
    "vo=map(lambda x:x.upper().replace(\"'\",\"\").replace(\"\\\"\",\"\").replace(\"?\",\"\").replace(\"~\",\"\"),v)\n",
    "for n in vo:\n",
    "    X_real.append(verbatim_to_char_seq(n, char_indices, maxLen))\n",
    "    \n",
    "X_real = np.array(X_real).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = model2.predict_classes(X_real, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pArray = map(lambda x:indices_label[x],p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('Data/verbatimResults.csv', 'w', encoding='ascii', errors='ignore') as f:\n",
    "    wr = csv.writer(f, dialect='excel')\n",
    "    for item in pArray:\n",
    "        wr.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
